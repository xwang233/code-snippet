{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: GeForce RTX 2070 SUPER \r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi -L | cut -d '(' -f 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "nb = 500\n",
    "\n",
    "def main(s: str):\n",
    "    def prof(b_, n_, dtype, f):\n",
    "        # print(b_, n_)\n",
    "        x = torch.randn(*b_, n_, n_, device='cuda', dtype=dtype)\n",
    "\n",
    "        xc = x.clone().cpu()\n",
    "\n",
    "        t1 = time.time()\n",
    "        for _ in range(nb):\n",
    "            yc = torch.inverse(xc)\n",
    "        t2 = time.time()\n",
    "        cpu_time = (t2-t1)/nb*1e3\n",
    "        # print('cpu', cpu_time, 'ms')\n",
    "\n",
    "        for _ in range(nb):\n",
    "            y = torch.inverse(x)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        c, d = torch.testing._compare_tensors_internal(xc.cuda(), x, rtol=1e-7, atol=1e-7, equal_nan=False)\n",
    "        if not c:\n",
    "            print('original matrix compare')\n",
    "            print(d)\n",
    "            raise RuntimeError('original value modified')\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        t1 = time.time()\n",
    "        for _ in range(nb):\n",
    "            y = torch.inverse(x)\n",
    "        torch.cuda.synchronize()\n",
    "        t2 = time.time()\n",
    "        gpu_time = (t2-t1)/nb*1e3\n",
    "        # print('gpu', gpu_time, 'ms')\n",
    "\n",
    "        a, b = torch.testing._compare_tensors_internal(yc.cuda(), y, rtol=1e-3, atol=1e-3, equal_nan=False)\n",
    "        if not a:\n",
    "            print('numerical mismatch: inverse value compare')\n",
    "            print(b)\n",
    "\n",
    "        print(f'{b_} {n_} {dtype}'.ljust(35) + f'{cpu_time : .3f}  {gpu_time : .3f}')\n",
    "        f.write(f'{b_} {n_} {dtype}; ' + f'{cpu_time : .3e}, {gpu_time : .3e}\\n')\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    print(s)\n",
    "    print(torch.__version__)\n",
    "    print()\n",
    "    print('batch_size, matrix_size, dtype'.ljust(35) + 'cpu_time(ms), gpu_time(ms)')\n",
    "    \n",
    "    shapes = itertools.product(\n",
    "        [[]] + [[2**x] for x in range(11)],\n",
    "        [2**i for i in range(1, 11)],\n",
    "        [torch.float]\n",
    "    )\n",
    "\n",
    "    with open(s+'.txt', 'w') as f:\n",
    "        for b, n, dtype in shapes:\n",
    "            if len(b) > 0 and b[0] * n >= 2**15:\n",
    "                continue\n",
    "            prof(b, n, dtype, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "1.7.0a0+05f0053\n",
      "\n",
      "batch_size, matrix_size, dtype     cpu_time(ms), gpu_time(ms)\n",
      "[] 2 torch.float32                  0.010   7.257\n",
      "[] 4 torch.float32                  0.010   7.281\n",
      "[] 8 torch.float32                  0.012   7.333\n",
      "[] 16 torch.float32                 0.017   7.382\n",
      "[] 32 torch.float32                 0.034   7.457\n",
      "[] 64 torch.float32                 0.071   8.207\n",
      "[] 128 torch.float32                0.361   8.005\n",
      "[] 256 torch.float32                1.062   11.516\n",
      "[] 512 torch.float32                5.277   14.600\n",
      "[] 1024 torch.float32               17.885   19.154\n",
      "[1] 2 torch.float32                 0.010   0.111\n",
      "[1] 4 torch.float32                 0.010   0.117\n",
      "[1] 8 torch.float32                 0.011   0.114\n",
      "[1] 16 torch.float32                0.016   0.121\n",
      "[1] 32 torch.float32                0.033   0.177\n",
      "[1] 64 torch.float32                0.071   0.421\n",
      "[1] 128 torch.float32               0.281   0.799\n",
      "[1] 256 torch.float32               1.080   1.662\n",
      "[1] 512 torch.float32               4.920   4.243\n",
      "[1] 1024 torch.float32              17.520   16.207\n",
      "[2] 2 torch.float32                 0.010   0.112\n",
      "[2] 4 torch.float32                 0.011   0.115\n",
      "[2] 8 torch.float32                 0.013   0.113\n",
      "[2] 16 torch.float32                0.021   0.119\n",
      "[2] 32 torch.float32                0.052   0.173\n",
      "[2] 64 torch.float32                0.130   0.426\n",
      "[2] 128 torch.float32               0.562   0.829\n",
      "[2] 256 torch.float32               2.045   1.742\n",
      "[2] 512 torch.float32               8.992   4.524\n",
      "[2] 1024 torch.float32              32.453   18.243\n",
      "[4] 2 torch.float32                 0.010   0.112\n",
      "[4] 4 torch.float32                 0.011   0.115\n",
      "[4] 8 torch.float32                 0.014   0.120\n",
      "[4] 16 torch.float32                0.030   0.120\n",
      "[4] 32 torch.float32                0.086   0.172\n",
      "[4] 64 torch.float32                0.222   0.432\n",
      "[4] 128 torch.float32               1.058   0.946\n",
      "[4] 256 torch.float32               4.124   1.799\n",
      "[4] 512 torch.float32               18.531   4.873\n",
      "[4] 1024 torch.float32              69.006   19.896\n",
      "[8] 2 torch.float32                 0.012   0.127\n",
      "[8] 4 torch.float32                 0.012   0.119\n",
      "[8] 8 torch.float32                 0.017   0.115\n",
      "[8] 16 torch.float32                0.046   0.120\n",
      "[8] 32 torch.float32                0.161   0.174\n",
      "[8] 64 torch.float32                0.429   0.434\n",
      "[8] 128 torch.float32               2.298   0.864\n",
      "[8] 256 torch.float32               7.802   1.888\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 107956 element(s) (out of 2097152) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.036670684814453125 (-16.142839431762695 vs. -16.106168746948242), which occurred at index (5, 380, 239).\n",
      "[8] 512 torch.float32               37.261   5.477\n",
      "[8] 1024 torch.float32              123.394   22.780\n",
      "[16] 2 torch.float32                0.012   0.113\n",
      "[16] 4 torch.float32                0.013   0.117\n",
      "[16] 8 torch.float32                0.024   0.115\n",
      "[16] 16 torch.float32               0.076   0.120\n",
      "[16] 32 torch.float32               0.312   0.179\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 3694 element(s) (out of 65536) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.814544677734375 (-510.1869812011719 vs. -511.00152587890625), which occurred at index (4, 62, 11).\n",
      "[16] 64 torch.float32               0.905   0.452\n",
      "[16] 128 torch.float32              3.982   0.923\n",
      "[16] 256 torch.float32              15.052   2.241\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 248571 element(s) (out of 4194304) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 1.15155029296875 (-280.2689208984375 vs. -281.42047119140625), which occurred at index (0, 368, 293).\n",
      "[16] 512 torch.float32              69.886   6.831\n",
      "[16] 1024 torch.float32             248.364   29.189\n",
      "[32] 2 torch.float32                0.014   0.112\n",
      "[32] 4 torch.float32                0.016   0.115\n",
      "[32] 8 torch.float32                0.038   0.113\n",
      "[32] 16 torch.float32               0.146   0.119\n",
      "[32] 32 torch.float32               0.616   0.178\n",
      "[32] 64 torch.float32               2.476   0.466\n",
      "[32] 128 torch.float32              8.266   1.016\n",
      "[32] 256 torch.float32              30.293   2.796\n",
      "[32] 512 torch.float32              146.874   8.983\n",
      "[64] 2 torch.float32                0.017   0.112\n",
      "[64] 4 torch.float32                0.022   0.116\n",
      "[64] 8 torch.float32                0.068   0.115\n",
      "[64] 16 torch.float32               0.273   0.120\n",
      "[64] 32 torch.float32               1.380   0.182\n",
      "[64] 64 torch.float32               3.446   0.503\n",
      "[64] 128 torch.float32              15.750   1.231\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 51628 element(s) (out of 4194304) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.11262130737304688 (-50.95573806762695 vs. -50.843116760253906), which occurred at index (54, 30, 244).\n",
      "[64] 256 torch.float32              60.343   3.864\n",
      "[128] 2 torch.float32               0.026   0.117\n",
      "[128] 4 torch.float32               0.035   0.118\n",
      "[128] 8 torch.float32               0.128   0.121\n",
      "[128] 16 torch.float32              0.546   0.125\n",
      "[128] 32 torch.float32              2.763   0.209\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 4053 element(s) (out of 524288) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 1.74200439453125 (-865.4207153320312 vs. -867.1627197265625), which occurred at index (11, 28, 8).\n",
      "[128] 64 torch.float32              6.866   0.621\n",
      "[128] 128 torch.float32             30.440   2.115\n",
      "[256] 2 torch.float32               0.058   0.122\n",
      "[256] 4 torch.float32               0.061   0.122\n",
      "[256] 8 torch.float32               0.247   0.127\n",
      "[256] 16 torch.float32              1.396   0.156\n",
      "[256] 32 torch.float32              5.108   0.266\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 4053 element(s) (out of 1048576) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 4.181884765625 (-1559.24169921875 vs. -1555.059814453125), which occurred at index (142, 13, 42).\n",
      "[256] 64 torch.float32              13.220   0.941\n",
      "[512] 2 torch.float32               0.076   0.154\n",
      "[512] 4 torch.float32               0.121   0.155\n",
      "[512] 8 torch.float32               0.496   0.159\n",
      "[512] 16 torch.float32              2.293   0.175\n",
      "[512] 32 torch.float32              9.785   0.339\n",
      "[1024] 2 torch.float32              0.140   0.175\n",
      "[1024] 4 torch.float32              0.227   0.177\n",
      "[1024] 8 torch.float32              1.057   0.184\n",
      "[1024] 16 torch.float32             4.478   0.216\n"
     ]
    }
   ],
   "source": [
    "main('before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after\n",
      "1.7.0a0+c98b599\n",
      "\n",
      "batch_size, matrix_size, dtype     cpu_time(ms), gpu_time(ms)\n",
      "[] 2 torch.float32                  0.010   0.116\n",
      "[] 4 torch.float32                  0.011   0.120\n",
      "[] 8 torch.float32                  0.012   0.128\n",
      "[] 16 torch.float32                 0.016   0.122\n",
      "[] 32 torch.float32                 0.032   0.178\n",
      "[] 64 torch.float32                 0.073   0.259\n",
      "[] 128 torch.float32                0.375   0.479\n",
      "[] 256 torch.float32                1.113   1.054\n",
      "[] 512 torch.float32                4.974   2.568\n",
      "[] 1024 torch.float32               19.628   6.901\n",
      "[1] 2 torch.float32                 0.009   0.169\n",
      "[1] 4 torch.float32                 0.010   0.160\n",
      "[1] 8 torch.float32                 0.011   0.162\n",
      "[1] 16 torch.float32                0.016   0.158\n",
      "[1] 32 torch.float32                0.032   0.208\n",
      "[1] 64 torch.float32                0.068   0.292\n",
      "[1] 128 torch.float32               0.344   0.521\n",
      "[1] 256 torch.float32               1.074   1.097\n",
      "[1] 512 torch.float32               4.288   2.612\n",
      "[1] 1024 torch.float32              17.290   6.938\n",
      "[2] 2 torch.float32                 0.010   0.188\n",
      "[2] 4 torch.float32                 0.011   0.186\n",
      "[2] 8 torch.float32                 0.013   0.189\n",
      "[2] 16 torch.float32                0.022   0.185\n",
      "[2] 32 torch.float32                0.049   0.271\n",
      "[2] 64 torch.float32                0.119   0.457\n",
      "[2] 128 torch.float32               0.651   0.902\n",
      "[2] 256 torch.float32               1.986   2.046\n",
      "[2] 512 torch.float32               9.279   5.104\n",
      "[2] 1024 torch.float32              31.877   13.747\n",
      "[4] 2 torch.float32                 0.010   0.233\n",
      "[4] 4 torch.float32                 0.011   0.235\n",
      "[4] 8 torch.float32                 0.014   0.240\n",
      "[4] 16 torch.float32                0.027   0.253\n",
      "[4] 32 torch.float32                0.082   0.421\n",
      "[4] 64 torch.float32                0.218   0.792\n",
      "[4] 128 torch.float32               1.189   1.716\n",
      "[4] 256 torch.float32               4.058   4.001\n",
      "[4] 512 torch.float32               18.149   10.059\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 762420 element(s) (out of 4194304) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.10302543640136719 (22.140045166015625 vs. 22.037019729614258), which occurred at index (1, 323, 856).\n",
      "[4] 1024 torch.float32              65.199   27.446\n",
      "[8] 2 torch.float32                 0.011   0.342\n",
      "[8] 4 torch.float32                 0.012   0.340\n",
      "[8] 8 torch.float32                 0.017   0.371\n",
      "[8] 16 torch.float32                0.044   0.390\n",
      "[8] 32 torch.float32                0.158   0.728\n",
      "[8] 64 torch.float32                0.428   1.465\n",
      "[8] 128 torch.float32               2.280   3.257\n",
      "[8] 256 torch.float32               7.928   7.862\n",
      "[8] 512 torch.float32               36.430   20.066\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 39 element(s) (out of 8388608) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.00391077995300293 (2.893357515335083 vs. 2.897268295288086), which occurred at index (3, 242, 455).\n",
      "[8] 1024 torch.float32              137.085   54.978\n",
      "[16] 2 torch.float32                0.013   0.098\n",
      "[16] 4 torch.float32                0.014   0.095\n",
      "[16] 8 torch.float32                0.026   0.107\n",
      "[16] 16 torch.float32               0.082   0.139\n",
      "[16] 32 torch.float32               0.328   0.172\n",
      "[16] 64 torch.float32               1.253   0.351\n",
      "[16] 128 torch.float32              4.356   1.088\n",
      "[16] 256 torch.float32              17.625   6.119\n",
      "[16] 512 torch.float32              72.239   40.040\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 997913 element(s) (out of 16777216) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.07090187072753906 (22.64145278930664 vs. 22.71235466003418), which occurred at index (11, 117, 263).\n",
      "[16] 1024 torch.float32             252.856   109.907\n",
      "[32] 2 torch.float32                0.020   0.094\n",
      "[32] 4 torch.float32                0.016   0.111\n",
      "[32] 8 torch.float32                0.038   0.106\n",
      "[32] 16 torch.float32               0.142   0.138\n",
      "[32] 32 torch.float32               0.609   0.178\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 3978 element(s) (out of 131072) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 1.33551025390625 (-658.314697265625 vs. -656.9791870117188), which occurred at index (28, 53, 44).\n",
      "[32] 64 torch.float32               1.774   0.330\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 12480 element(s) (out of 524288) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.16489410400390625 (94.62999725341797 vs. 94.46510314941406), which occurred at index (18, 28, 106).\n",
      "[32] 128 torch.float32              8.164   1.089\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 13369 element(s) (out of 2097152) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.043117523193359375 (34.69142532348633 vs. 34.64830780029297), which occurred at index (2, 30, 77).\n",
      "[32] 256 torch.float32              30.019   7.280\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 35517 element(s) (out of 8388608) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.020778656005859375 (12.641467094421387 vs. 12.662245750427246), which occurred at index (11, 205, 414).\n",
      "[32] 512 torch.float32              150.177   79.942\n",
      "[64] 2 torch.float32                0.017   0.092\n",
      "[64] 4 torch.float32                0.022   0.095\n",
      "[64] 8 torch.float32                0.069   0.101\n",
      "[64] 16 torch.float32               0.275   0.135\n",
      "[64] 32 torch.float32               1.598   0.178\n",
      "[64] 64 torch.float32               3.869   0.383\n",
      "[64] 128 torch.float32              16.500   1.469\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 31785 element(s) (out of 4194304) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.1166839599609375 (-100.70169067382812 vs. -100.81837463378906), which occurred at index (54, 55, 57).\n",
      "[64] 256 torch.float32              62.566   11.215\n",
      "[128] 2 torch.float32               0.026   0.095\n",
      "[128] 4 torch.float32               0.035   0.098\n",
      "[128] 8 torch.float32               0.130   0.101\n",
      "[128] 16 torch.float32              0.557   0.139\n",
      "[128] 32 torch.float32              3.031   0.199\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 7459 element(s) (out of 524288) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 19.85498046875 (2470.613525390625 vs. 2450.758544921875), which occurred at index (16, 39, 29).\n",
      "[128] 64 torch.float32              6.714   0.481\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 16153 element(s) (out of 2097152) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 1.269561767578125 (-346.3931579589844 vs. -345.12359619140625), which occurred at index (83, 114, 93).\n",
      "[128] 128 torch.float32             31.990   2.652\n",
      "[256] 2 torch.float32               0.041   0.094\n",
      "[256] 4 torch.float32               0.062   0.098\n",
      "[256] 8 torch.float32               0.249   0.103\n",
      "[256] 16 torch.float32              1.138   0.136\n",
      "[256] 32 torch.float32              4.963   0.241\n",
      "[256] 64 torch.float32              13.698   0.781\n",
      "[512] 2 torch.float32               0.072   0.094\n",
      "[512] 4 torch.float32               0.114   0.095\n",
      "[512] 8 torch.float32               0.485   0.104\n",
      "[512] 16 torch.float32              2.761   0.148\n",
      "[512] 32 torch.float32              9.913   0.347\n",
      "[1024] 2 torch.float32              0.137   0.098\n",
      "[1024] 4 torch.float32              0.219   0.099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1024] 8 torch.float32              1.001   0.105\n",
      "[1024] 16 torch.float32             4.317   0.173\n"
     ]
    }
   ],
   "source": [
    "main('after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape                      cpu_time, gpu_time_before (magma), gpu_time_after (cusolver/cublas)\n",
      "[] 2 torch.float32          0.010,  7.257,  0.116 \n",
      "[] 4 torch.float32          0.010,  7.281,  0.120 \n",
      "[] 8 torch.float32          0.012,  7.333,  0.128 \n",
      "[] 16 torch.float32         0.016,  7.382,  0.122 \n",
      "[] 32 torch.float32         0.033,  7.457,  0.178 \n",
      "[] 64 torch.float32         0.072,  8.207,  0.259 \n",
      "[] 128 torch.float32        0.368,  8.005,  0.478 \n",
      "[] 256 torch.float32        1.087,  11.520,  1.054 \n",
      "[] 512 torch.float32        5.126,  14.600,  2.568 \n",
      "[] 1024 torch.float32       18.755,  19.150,  6.901 \n",
      "[1] 2 torch.float32         0.009,  0.111,  0.169 ******************** regressed\n",
      "[1] 4 torch.float32         0.010,  0.117,  0.160 ******************** regressed\n",
      "[1] 8 torch.float32         0.011,  0.114,  0.162 ******************** regressed\n",
      "[1] 16 torch.float32        0.016,  0.121,  0.158 ******************** regressed\n",
      "[1] 32 torch.float32        0.033,  0.177,  0.208 ******************** regressed\n",
      "[1] 64 torch.float32        0.070,  0.421,  0.292 \n",
      "[1] 128 torch.float32       0.312,  0.799,  0.521 \n",
      "[1] 256 torch.float32       1.077,  1.662,  1.097 \n",
      "[1] 512 torch.float32       4.604,  4.243,  2.612 \n",
      "[1] 1024 torch.float32      17.405,  16.210,  6.938 \n",
      "[2] 2 torch.float32         0.010,  0.112,  0.189 ******************** regressed\n",
      "[2] 4 torch.float32         0.011,  0.115,  0.186 ******************** regressed\n",
      "[2] 8 torch.float32         0.013,  0.113,  0.189 ******************** regressed\n",
      "[2] 16 torch.float32        0.021,  0.119,  0.185 ******************** regressed\n",
      "[2] 32 torch.float32        0.050,  0.173,  0.271 ******************** regressed\n",
      "[2] 64 torch.float32        0.124,  0.426,  0.457 ******************** regressed\n",
      "[2] 128 torch.float32       0.607,  0.829,  0.902 ******************** regressed\n",
      "[2] 256 torch.float32       2.015,  1.742,  2.046 ******************** regressed\n",
      "[2] 512 torch.float32       9.136,  4.524,  5.104 ******************** regressed\n",
      "[2] 1024 torch.float32      32.165,  18.240,  13.750 \n",
      "[4] 2 torch.float32         0.010,  0.112,  0.233 ******************** regressed\n",
      "[4] 4 torch.float32         0.011,  0.115,  0.235 ******************** regressed\n",
      "[4] 8 torch.float32         0.014,  0.120,  0.240 ******************** regressed\n",
      "[4] 16 torch.float32        0.028,  0.120,  0.253 ******************** regressed\n",
      "[4] 32 torch.float32        0.084,  0.172,  0.421 ******************** regressed\n",
      "[4] 64 torch.float32        0.220,  0.432,  0.792 ******************** regressed\n",
      "[4] 128 torch.float32       1.123,  0.946,  1.716 ******************** regressed\n",
      "[4] 256 torch.float32       4.091,  1.799,  4.001 ******************** regressed\n",
      "[4] 512 torch.float32       18.340,  4.873,  10.060 ******************** regressed\n",
      "[4] 1024 torch.float32      67.105,  19.900,  27.450 ******************** regressed\n",
      "[8] 2 torch.float32         0.011,  0.127,  0.342 ******************** regressed\n",
      "[8] 4 torch.float32         0.012,  0.119,  0.341 ******************** regressed\n",
      "[8] 8 torch.float32         0.017,  0.115,  0.371 ******************** regressed\n",
      "[8] 16 torch.float32        0.045,  0.120,  0.390 ******************** regressed\n",
      "[8] 32 torch.float32        0.160,  0.174,  0.728 ******************** regressed\n",
      "[8] 64 torch.float32        0.429,  0.434,  1.465 ******************** regressed\n",
      "[8] 128 torch.float32       2.289,  0.864,  3.257 ******************** regressed\n",
      "[8] 256 torch.float32       7.865,  1.888,  7.862 ******************** regressed\n",
      "[8] 512 torch.float32       36.845,  5.477,  20.070 ******************** regressed\n",
      "[8] 1024 torch.float32      130.250,  22.780,  54.980 ******************** regressed\n",
      "[16] 2 torch.float32        0.012,  0.113,  0.098 \n",
      "[16] 4 torch.float32        0.013,  0.117,  0.095 \n",
      "[16] 8 torch.float32        0.025,  0.116,  0.107 \n",
      "[16] 16 torch.float32       0.079,  0.120,  0.139 ******************** regressed\n",
      "[16] 32 torch.float32       0.320,  0.179,  0.172 \n",
      "[16] 64 torch.float32       1.079,  0.452,  0.351 \n",
      "[16] 128 torch.float32      4.169,  0.923,  1.088 ******************** regressed\n",
      "[16] 256 torch.float32      16.340,  2.241,  6.119 ******************** regressed\n",
      "[16] 512 torch.float32      71.065,  6.831,  40.040 ******************** regressed\n",
      "[16] 1024 torch.float32     250.650,  29.190,  109.900 ******************** regressed\n",
      "[32] 2 torch.float32        0.017,  0.112,  0.094 \n",
      "[32] 4 torch.float32        0.016,  0.115,  0.111 \n",
      "[32] 8 torch.float32        0.038,  0.113,  0.106 \n",
      "[32] 16 torch.float32       0.144,  0.119,  0.138 ******************** regressed\n",
      "[32] 32 torch.float32       0.612,  0.178,  0.178 ******************** regressed\n",
      "[32] 64 torch.float32       2.125,  0.466,  0.330 \n",
      "[32] 128 torch.float32      8.215,  1.016,  1.089 ******************** regressed\n",
      "[32] 256 torch.float32      30.155,  2.796,  7.280 ******************** regressed\n",
      "[32] 512 torch.float32      148.550,  8.983,  79.940 ******************** regressed\n",
      "[64] 2 torch.float32        0.017,  0.112,  0.092 \n",
      "[64] 4 torch.float32        0.022,  0.116,  0.095 \n",
      "[64] 8 torch.float32        0.068,  0.115,  0.101 \n",
      "[64] 16 torch.float32       0.274,  0.120,  0.135 ******************** regressed\n",
      "[64] 32 torch.float32       1.489,  0.182,  0.178 \n",
      "[64] 64 torch.float32       3.658,  0.502,  0.383 \n",
      "[64] 128 torch.float32      16.125,  1.231,  1.469 ******************** regressed\n",
      "[64] 256 torch.float32      61.455,  3.864,  11.210 ******************** regressed\n",
      "[128] 2 torch.float32       0.026,  0.117,  0.095 \n",
      "[128] 4 torch.float32       0.035,  0.118,  0.098 \n",
      "[128] 8 torch.float32       0.129,  0.121,  0.101 \n",
      "[128] 16 torch.float32      0.551,  0.125,  0.139 ******************** regressed\n",
      "[128] 32 torch.float32      2.897,  0.209,  0.199 \n",
      "[128] 64 torch.float32      6.790,  0.621,  0.481 \n",
      "[128] 128 torch.float32     31.215,  2.115,  2.652 ******************** regressed\n",
      "[256] 2 torch.float32       0.050,  0.122,  0.094 \n",
      "[256] 4 torch.float32       0.061,  0.122,  0.098 \n",
      "[256] 8 torch.float32       0.248,  0.127,  0.103 \n",
      "[256] 16 torch.float32      1.267,  0.156,  0.136 \n",
      "[256] 32 torch.float32      5.035,  0.267,  0.241 \n",
      "[256] 64 torch.float32      13.460,  0.941,  0.781 \n",
      "[512] 2 torch.float32       0.074,  0.154,  0.094 \n",
      "[512] 4 torch.float32       0.117,  0.155,  0.095 \n",
      "[512] 8 torch.float32       0.491,  0.159,  0.104 \n",
      "[512] 16 torch.float32      2.527,  0.175,  0.148 \n",
      "[512] 32 torch.float32      9.849,  0.339,  0.347 ******************** regressed\n",
      "[1024] 2 torch.float32      0.138,  0.175,  0.098 \n",
      "[1024] 4 torch.float32      0.223,  0.177,  0.099 \n",
      "[1024] 8 torch.float32      1.029,  0.184,  0.105 \n",
      "[1024] 16 torch.float32     4.397,  0.216,  0.173 \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def readfile(fn):\n",
    "    with open(fn, 'r') as f:\n",
    "        fl = f.readlines()\n",
    "    \n",
    "    dc = {}\n",
    "    dg = {}\n",
    "    for _line in fl:\n",
    "        key, cpu_time, gpu_time = re.split(';|,', _line.rstrip())\n",
    "        dc[key] = float(cpu_time)\n",
    "        dg[key] = float(gpu_time)\n",
    "    \n",
    "    return (dc, dg)\n",
    "\n",
    "def compare():\n",
    "    print('shape'.ljust(26), 'cpu_time, gpu_time_before (magma), gpu_time_after (cusolver/cublas)')\n",
    "    dc_b, dg_b = readfile('before.txt')\n",
    "    dc_a, dg_a = readfile('after.txt')\n",
    "    \n",
    "    for key in dc_b:\n",
    "        cpu_time = 0.5 * (dc_b[key] + dc_a[key])\n",
    "        gpu_time_before = dg_b[key]\n",
    "        gpu_time_after = dg_a[key]\n",
    "        \n",
    "        if gpu_time_after > gpu_time_before:\n",
    "            gs = '*' * 20 + ' regressed'\n",
    "        else:\n",
    "            gs = ''\n",
    "\n",
    "        print(f'{key: <26} {cpu_time: .3f}, {gpu_time_before: .3f}, {gpu_time_after: .3f} {gs}')\n",
    "\n",
    "compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitfce950e88ea94256bae6c6f663f53e68"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
