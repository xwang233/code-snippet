{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: GeForce RTX 2070 SUPER \r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi -L | cut -d '(' -f 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "nb = 500\n",
    "\n",
    "def main(s: str):\n",
    "    def prof(b_, n_, dtype, f):\n",
    "        # print(b_, n_)\n",
    "        x = torch.randn(*b_, n_, n_, device='cuda', dtype=dtype)\n",
    "\n",
    "        xc = x.clone().cpu()\n",
    "\n",
    "        t1 = time.time()\n",
    "        for _ in range(nb):\n",
    "            yc = torch.inverse(xc)\n",
    "        t2 = time.time()\n",
    "        cpu_time = (t2-t1)/nb*1e3\n",
    "        # print('cpu', cpu_time, 'ms')\n",
    "\n",
    "        for _ in range(nb):\n",
    "            y = torch.inverse(x)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        c, d = torch.testing._compare_tensors_internal(xc.cuda(), x, rtol=1e-7, atol=1e-7, equal_nan=False)\n",
    "        if not c:\n",
    "            print('original matrix compare')\n",
    "            print(d)\n",
    "            raise RuntimeError('original value modified')\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        t1 = time.time()\n",
    "        for _ in range(nb):\n",
    "            y = torch.inverse(x)\n",
    "        torch.cuda.synchronize()\n",
    "        t2 = time.time()\n",
    "        gpu_time = (t2-t1)/nb*1e3\n",
    "        # print('gpu', gpu_time, 'ms')\n",
    "\n",
    "        a, b = torch.testing._compare_tensors_internal(yc.cuda(), y, rtol=1e-3, atol=1e-3, equal_nan=False)\n",
    "        if not a:\n",
    "            print('numerical mismatch: inverse value compare')\n",
    "            print(b)\n",
    "\n",
    "        print(f'{b_} {n_} {dtype}'.ljust(35) + f'{cpu_time : .3f}  {gpu_time : .3f}')\n",
    "        f.write(f'{b_} {n_} {dtype}; ' + f'{cpu_time : .3e}, {gpu_time : .3e}\\n')\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    print(s)\n",
    "    print(torch.__version__)\n",
    "    print()\n",
    "    print('batch_size, matrix_size, dtype'.ljust(35) + 'cpu_time(ms), gpu_time(ms)')\n",
    "    \n",
    "    shapes = itertools.product(\n",
    "        [[]] + [[2**x] for x in range(11)],\n",
    "        [2**i for i in range(1, 11)],\n",
    "        [torch.float]\n",
    "    )\n",
    "\n",
    "    with open(s+'.txt', 'w') as f:\n",
    "        for b, n, dtype in shapes:\n",
    "            if len(b) > 0 and b[0] * n >= 2**15:\n",
    "                continue\n",
    "            prof(b, n, dtype, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "1.7.0a0+4ae832e\n",
      "\n",
      "batch_size, matrix_size, dtype     cpu_time(ms), gpu_time(ms)\n",
      "[] 2 torch.float32                  0.018   7.370\n",
      "[] 4 torch.float32                  0.011   7.422\n",
      "[] 8 torch.float32                  0.011   7.436\n",
      "[] 16 torch.float32                 0.056   7.513\n",
      "[] 32 torch.float32                 0.179   7.616\n",
      "[] 64 torch.float32                 0.205   7.721\n",
      "[] 128 torch.float32                0.395   8.000\n",
      "[] 256 torch.float32                1.115   11.267\n",
      "[] 512 torch.float32                4.567   14.267\n",
      "[] 1024 torch.float32               19.247   20.489\n",
      "[1] 2 torch.float32                 0.009   0.113\n",
      "[1] 4 torch.float32                 0.010   0.112\n",
      "[1] 8 torch.float32                 0.011   0.116\n",
      "[1] 16 torch.float32                0.016   0.128\n",
      "[1] 32 torch.float32                0.032   0.179\n",
      "[1] 64 torch.float32                0.072   0.427\n",
      "[1] 128 torch.float32               0.353   0.806\n",
      "[1] 256 torch.float32               1.214   1.687\n",
      "[1] 512 torch.float32               4.422   4.268\n",
      "[1] 1024 torch.float32              16.698   16.410\n",
      "[2] 2 torch.float32                 0.009   0.111\n",
      "[2] 4 torch.float32                 0.010   0.115\n",
      "[2] 8 torch.float32                 0.012   0.115\n",
      "[2] 16 torch.float32                0.021   0.119\n",
      "[2] 32 torch.float32                0.050   0.170\n",
      "[2] 64 torch.float32                0.121   0.426\n",
      "[2] 128 torch.float32               0.662   0.828\n",
      "[2] 256 torch.float32               2.085   1.747\n",
      "[2] 512 torch.float32               8.766   4.537\n",
      "[2] 1024 torch.float32              32.368   18.264\n",
      "[4] 2 torch.float32                 0.010   0.110\n",
      "[4] 4 torch.float32                 0.011   0.116\n",
      "[4] 8 torch.float32                 0.014   0.114\n",
      "[4] 16 torch.float32                0.028   0.121\n",
      "[4] 32 torch.float32                0.084   0.170\n",
      "[4] 64 torch.float32                0.228   0.430\n",
      "[4] 128 torch.float32               1.240   0.844\n",
      "[4] 256 torch.float32               4.135   1.816\n",
      "[4] 512 torch.float32               18.453   4.928\n",
      "[4] 1024 torch.float32              66.248   19.946\n",
      "[8] 2 torch.float32                 0.011   0.111\n",
      "[8] 4 torch.float32                 0.011   0.116\n",
      "[8] 8 torch.float32                 0.017   0.114\n",
      "[8] 16 torch.float32                0.044   0.123\n",
      "[8] 32 torch.float32                0.161   0.175\n",
      "[8] 64 torch.float32                0.448   0.439\n",
      "[8] 128 torch.float32               2.246   1.056\n",
      "[8] 256 torch.float32               8.057   2.359\n",
      "[8] 512 torch.float32               36.753   5.573\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 3 element(s) (out of 8388608) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.004320383071899414 (-3.2422828674316406 vs. -3.237962484359741), which occurred at index (4, 139, 830).\n",
      "[8] 1024 torch.float32              142.390   23.290\n",
      "[16] 2 torch.float32                0.012   0.116\n",
      "[16] 4 torch.float32                0.012   0.117\n",
      "[16] 8 torch.float32                0.024   0.116\n",
      "[16] 16 torch.float32               0.077   0.121\n",
      "[16] 32 torch.float32               0.309   0.180\n",
      "[16] 64 torch.float32               1.124   0.528\n",
      "[16] 128 torch.float32              4.193   1.116\n",
      "[16] 256 torch.float32              15.591   2.722\n",
      "[16] 512 torch.float32              74.277   6.747\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 361203 element(s) (out of 16777216) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.022258758544921875 (-6.525774955749512 vs. -6.548033714294434), which occurred at index (5, 627, 571).\n",
      "[16] 1024 torch.float32             277.386   28.982\n",
      "[32] 2 torch.float32                0.014   0.110\n",
      "[32] 4 torch.float32                0.015   0.113\n",
      "[32] 8 torch.float32                0.038   0.115\n",
      "[32] 16 torch.float32               0.144   0.121\n",
      "[32] 32 torch.float32               0.605   0.177\n",
      "[32] 64 torch.float32               2.585   0.467\n",
      "[32] 128 torch.float32              8.297   1.213\n",
      "[32] 256 torch.float32              31.814   2.775\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 207173 element(s) (out of 8388608) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.10821151733398438 (-37.831199645996094 vs. -37.72298812866211), which occurred at index (28, 511, 259).\n",
      "[32] 512 torch.float32              149.213   8.966\n",
      "[64] 2 torch.float32                0.017   0.112\n",
      "[64] 4 torch.float32                0.022   0.115\n",
      "[64] 8 torch.float32                0.067   0.115\n",
      "[64] 16 torch.float32               0.274   0.123\n",
      "[64] 32 torch.float32               1.304   0.184\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 3083 element(s) (out of 262144) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 1.1473388671875 (1064.2474365234375 vs. 1065.394775390625), which occurred at index (11, 49, 28).\n",
      "[64] 64 torch.float32               3.536   0.504\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 16384 element(s) (out of 1048576) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 1109.2060546875 (-8792.9296875 vs. -9902.1357421875), which occurred at index (23, 37, 124).\n",
      "[64] 128 torch.float32              15.889   1.221\n",
      "[64] 256 torch.float32              63.083   3.827\n",
      "[128] 2 torch.float32               0.026   0.127\n",
      "[128] 4 torch.float32               0.039   0.123\n",
      "[128] 8 torch.float32               0.130   0.121\n",
      "[128] 16 torch.float32              0.539   0.125\n",
      "[128] 32 torch.float32              2.947   0.211\n",
      "[128] 64 torch.float32              6.603   0.564\n",
      "[128] 128 torch.float32             31.751   1.744\n",
      "[256] 2 torch.float32               0.041   0.116\n",
      "[256] 4 torch.float32               0.060   0.118\n",
      "[256] 8 torch.float32               0.246   0.124\n",
      "[256] 16 torch.float32              1.271   0.135\n",
      "[256] 32 torch.float32              5.673   0.246\n",
      "[256] 64 torch.float32              13.478   0.854\n",
      "[512] 2 torch.float32               0.072   0.131\n",
      "[512] 4 torch.float32               0.115   0.130\n",
      "[512] 8 torch.float32               0.487   0.134\n",
      "[512] 16 torch.float32              3.199   0.178\n",
      "[512] 32 torch.float32              10.466   0.312\n",
      "[1024] 2 torch.float32              0.133   0.147\n",
      "[1024] 4 torch.float32              0.224   0.151\n",
      "[1024] 8 torch.float32              1.196   0.185\n",
      "[1024] 16 torch.float32             4.537   0.218\n"
     ]
    }
   ],
   "source": [
    "main('before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after\n",
      "1.7.0a0+5d46ad2\n",
      "\n",
      "batch_size, matrix_size, dtype     cpu_time(ms), gpu_time(ms)\n",
      "[] 2 torch.float32                  0.011   0.145\n",
      "[] 4 torch.float32                  0.014   0.147\n",
      "[] 8 torch.float32                  0.014   0.141\n",
      "[] 16 torch.float32                 0.018   0.123\n",
      "[] 32 torch.float32                 0.034   0.172\n",
      "[] 64 torch.float32                 0.083   0.253\n",
      "[] 128 torch.float32                0.367   0.478\n",
      "[] 256 torch.float32                1.142   1.060\n",
      "[] 512 torch.float32                4.989   2.564\n",
      "[] 1024 torch.float32               17.907   6.908\n",
      "[1] 2 torch.float32                 0.010   0.148\n",
      "[1] 4 torch.float32                 0.011   0.149\n",
      "[1] 8 torch.float32                 0.012   0.147\n",
      "[1] 16 torch.float32                0.017   0.137\n",
      "[1] 32 torch.float32                0.033   0.188\n",
      "[1] 64 torch.float32                0.071   0.282\n",
      "[1] 128 torch.float32               0.369   0.498\n",
      "[1] 256 torch.float32               1.299   1.079\n",
      "[1] 512 torch.float32               4.735   2.610\n",
      "[1] 1024 torch.float32              16.169   6.903\n",
      "[2] 2 torch.float32                 0.010   0.195\n",
      "[2] 4 torch.float32                 0.028   0.210\n",
      "[2] 8 torch.float32                 0.014   0.192\n",
      "[2] 16 torch.float32                0.020   0.183\n",
      "[2] 32 torch.float32                0.051   0.252\n",
      "[2] 64 torch.float32                0.122   0.376\n",
      "[2] 128 torch.float32               0.730   0.680\n",
      "[2] 256 torch.float32               2.156   1.436\n",
      "[2] 512 torch.float32               8.820   3.519\n",
      "[2] 1024 torch.float32              31.017   12.186\n",
      "[4] 2 torch.float32                 0.010   0.254\n",
      "[4] 4 torch.float32                 0.011   0.265\n",
      "[4] 8 torch.float32                 0.014   0.253\n",
      "[4] 16 torch.float32                0.028   0.227\n",
      "[4] 32 torch.float32                0.086   0.341\n",
      "[4] 64 torch.float32                0.228   0.522\n",
      "[4] 128 torch.float32               1.194   0.952\n",
      "[4] 256 torch.float32               4.288   2.063\n",
      "[4] 512 torch.float32               17.948   5.039\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 1 element(s) (out of 4194304) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.0013650059700012207 (-0.28345727920532227 vs. -0.2848222851753235), which occurred at index (2, 594, 243).\n",
      "[4] 1024 torch.float32              64.134   21.633\n",
      "[8] 2 torch.float32                 0.011   0.402\n",
      "[8] 4 torch.float32                 0.013   0.385\n",
      "[8] 8 torch.float32                 0.018   0.384\n",
      "[8] 16 torch.float32                0.044   0.344\n",
      "[8] 32 torch.float32                0.166   0.525\n",
      "[8] 64 torch.float32                0.445   0.815\n",
      "[8] 128 torch.float32               2.387   1.473\n",
      "[8] 256 torch.float32               8.025   3.173\n",
      "[8] 512 torch.float32               37.415   7.825\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 513688 element(s) (out of 8388608) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.03337812423706055 (-7.010975360870361 vs. -7.044353485107422), which occurred at index (2, 167, 46).\n",
      "[8] 1024 torch.float32              137.258   38.612\n",
      "[16] 2 torch.float32                0.012   0.117\n",
      "[16] 4 torch.float32                0.013   0.117\n",
      "[16] 8 torch.float32                0.025   0.127\n",
      "[16] 16 torch.float32               0.077   0.159\n",
      "[16] 32 torch.float32               0.315   0.194\n",
      "[16] 64 torch.float32               1.119   0.403\n",
      "[16] 128 torch.float32              4.606   1.083\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 39409 element(s) (out of 1048576) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.11627197265625 (-80.95758056640625 vs. -80.84130859375), which occurred at index (15, 79, 223).\n",
      "[16] 256 torch.float32              15.598   6.191\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 256196 element(s) (out of 4194304) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 2.9341583251953125 (-148.88856506347656 vs. -151.82272338867188), which occurred at index (2, 373, 112).\n",
      "[16] 512 torch.float32              72.095   13.318\n",
      "[16] 1024 torch.float32             273.743   71.929\n",
      "[32] 2 torch.float32                0.015   0.119\n",
      "[32] 4 torch.float32                0.016   0.122\n",
      "[32] 8 torch.float32                0.039   0.123\n",
      "[32] 16 torch.float32               0.143   0.158\n",
      "[32] 32 torch.float32               0.619   0.194\n",
      "[32] 64 torch.float32               1.774   0.354\n",
      "[32] 128 torch.float32              8.555   1.109\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 47212 element(s) (out of 2097152) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.1363677978515625 (78.9259033203125 vs. 78.78953552246094), which occurred at index (15, 90, 201).\n",
      "[32] 256 torch.float32              31.885   7.290\n",
      "[32] 512 torch.float32              154.359   24.542\n",
      "[64] 2 torch.float32                0.018   0.118\n",
      "[64] 4 torch.float32                0.023   0.120\n",
      "[64] 8 torch.float32                0.069   0.125\n",
      "[64] 16 torch.float32               0.271   0.158\n",
      "[64] 32 torch.float32               1.565   0.203\n",
      "[64] 64 torch.float32               4.172   0.389\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 16307 element(s) (out of 1048576) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 4.1673583984375 (-763.6556396484375 vs. -767.822998046875), which occurred at index (59, 125, 62).\n",
      "[64] 128 torch.float32              16.469   1.489\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 71600 element(s) (out of 4194304) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.5872650146484375 (155.01910400390625 vs. 154.4318389892578), which occurred at index (27, 215, 105).\n",
      "[64] 256 torch.float32              69.100   11.369\n",
      "[128] 2 torch.float32               0.030   0.161\n",
      "[128] 4 torch.float32               0.041   0.154\n",
      "[128] 8 torch.float32               0.136   0.154\n",
      "[128] 16 torch.float32              0.550   0.188\n",
      "[128] 32 torch.float32              2.734   0.252\n",
      "[128] 64 torch.float32              6.973   0.541\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 16107 element(s) (out of 2097152) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 2.721343994140625 (-416.978759765625 vs. -419.7001037597656), which occurred at index (40, 67, 53).\n",
      "[128] 128 torch.float32             39.765   2.676\n",
      "[256] 2 torch.float32               0.042   0.121\n",
      "[256] 4 torch.float32               0.063   0.121\n",
      "[256] 8 torch.float32               0.249   0.128\n",
      "[256] 16 torch.float32              1.279   0.163\n",
      "numerical mismatch: inverse value compare\n",
      "With rtol=0.001 and atol=0.001, found 901 element(s) (out of 262144) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 1.5299072265625 (1396.5003662109375 vs. 1394.970458984375), which occurred at index (199, 17, 16).\n",
      "[256] 32 torch.float32              4.931   0.264\n",
      "[256] 64 torch.float32              12.821   0.832\n",
      "[512] 2 torch.float32               0.073   0.123\n",
      "[512] 4 torch.float32               0.116   0.123\n",
      "[512] 8 torch.float32               0.493   0.131\n",
      "[512] 16 torch.float32              3.005   0.174\n",
      "[512] 32 torch.float32              9.720   0.365\n",
      "[1024] 2 torch.float32              0.135   0.129\n",
      "[1024] 4 torch.float32              0.223   0.128\n",
      "[1024] 8 torch.float32              0.990   0.135\n",
      "[1024] 16 torch.float32             4.379   0.206\n"
     ]
    }
   ],
   "source": [
    "main('after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape                      cpu_time, gpu_time_before (magma), gpu_time_after (cusolver/cublas)\n",
      "[] 2 torch.float32          0.014,  7.370,  0.145 \n",
      "[] 4 torch.float32          0.012,  7.422,  0.147 \n",
      "[] 8 torch.float32          0.013,  7.436,  0.141 \n",
      "[] 16 torch.float32         0.037,  7.513,  0.123 \n",
      "[] 32 torch.float32         0.106,  7.616,  0.172 \n",
      "[] 64 torch.float32         0.144,  7.721,  0.253 \n",
      "[] 128 torch.float32        0.381,  8.000,  0.478 \n",
      "[] 256 torch.float32        1.128,  11.270,  1.060 \n",
      "[] 512 torch.float32        4.778,  14.270,  2.564 \n",
      "[] 1024 torch.float32       18.580,  20.490,  6.908 \n",
      "[1] 2 torch.float32         0.010,  0.113,  0.148 ******************** regressed\n",
      "[1] 4 torch.float32         0.010,  0.112,  0.149 ******************** regressed\n",
      "[1] 8 torch.float32         0.012,  0.116,  0.147 ******************** regressed\n",
      "[1] 16 torch.float32        0.016,  0.128,  0.137 ******************** regressed\n",
      "[1] 32 torch.float32        0.032,  0.179,  0.188 ******************** regressed\n",
      "[1] 64 torch.float32        0.071,  0.427,  0.281 \n",
      "[1] 128 torch.float32       0.361,  0.805,  0.498 \n",
      "[1] 256 torch.float32       1.256,  1.687,  1.079 \n",
      "[1] 512 torch.float32       4.579,  4.268,  2.610 \n",
      "[1] 1024 torch.float32      16.435,  16.410,  6.903 \n",
      "[2] 2 torch.float32         0.010,  0.111,  0.195 ******************** regressed\n",
      "[2] 4 torch.float32         0.019,  0.115,  0.210 ******************** regressed\n",
      "[2] 8 torch.float32         0.013,  0.115,  0.193 ******************** regressed\n",
      "[2] 16 torch.float32        0.021,  0.119,  0.183 ******************** regressed\n",
      "[2] 32 torch.float32        0.051,  0.170,  0.252 ******************** regressed\n",
      "[2] 64 torch.float32        0.122,  0.426,  0.376 \n",
      "[2] 128 torch.float32       0.696,  0.828,  0.680 \n",
      "[2] 256 torch.float32       2.120,  1.747,  1.436 \n",
      "[2] 512 torch.float32       8.793,  4.537,  3.519 \n",
      "[2] 1024 torch.float32      31.695,  18.260,  12.190 \n",
      "[4] 2 torch.float32         0.010,  0.110,  0.254 ******************** regressed\n",
      "[4] 4 torch.float32         0.011,  0.116,  0.265 ******************** regressed\n",
      "[4] 8 torch.float32         0.014,  0.114,  0.253 ******************** regressed\n",
      "[4] 16 torch.float32        0.028,  0.121,  0.227 ******************** regressed\n",
      "[4] 32 torch.float32        0.085,  0.170,  0.341 ******************** regressed\n",
      "[4] 64 torch.float32        0.228,  0.430,  0.522 ******************** regressed\n",
      "[4] 128 torch.float32       1.217,  0.844,  0.952 ******************** regressed\n",
      "[4] 256 torch.float32       4.212,  1.816,  2.063 ******************** regressed\n",
      "[4] 512 torch.float32       18.200,  4.928,  5.039 ******************** regressed\n",
      "[4] 1024 torch.float32      65.190,  19.950,  21.630 ******************** regressed\n",
      "[8] 2 torch.float32         0.011,  0.111,  0.402 ******************** regressed\n",
      "[8] 4 torch.float32         0.012,  0.116,  0.385 ******************** regressed\n",
      "[8] 8 torch.float32         0.018,  0.114,  0.384 ******************** regressed\n",
      "[8] 16 torch.float32        0.044,  0.123,  0.344 ******************** regressed\n",
      "[8] 32 torch.float32        0.164,  0.175,  0.525 ******************** regressed\n",
      "[8] 64 torch.float32        0.446,  0.439,  0.816 ******************** regressed\n",
      "[8] 128 torch.float32       2.317,  1.056,  1.473 ******************** regressed\n",
      "[8] 256 torch.float32       8.041,  2.359,  3.173 ******************** regressed\n",
      "[8] 512 torch.float32       37.080,  5.573,  7.825 ******************** regressed\n",
      "[8] 1024 torch.float32      139.850,  23.290,  38.610 ******************** regressed\n",
      "[16] 2 torch.float32        0.012,  0.116,  0.117 ******************** regressed\n",
      "[16] 4 torch.float32        0.013,  0.117,  0.117 ******************** regressed\n",
      "[16] 8 torch.float32        0.024,  0.116,  0.127 ******************** regressed\n",
      "[16] 16 torch.float32       0.077,  0.121,  0.159 ******************** regressed\n",
      "[16] 32 torch.float32       0.312,  0.180,  0.194 ******************** regressed\n",
      "[16] 64 torch.float32       1.122,  0.528,  0.403 \n",
      "[16] 128 torch.float32      4.399,  1.116,  1.083 \n",
      "[16] 256 torch.float32      15.595,  2.722,  6.191 ******************** regressed\n",
      "[16] 512 torch.float32      73.190,  6.747,  13.320 ******************** regressed\n",
      "[16] 1024 torch.float32     275.550,  28.980,  71.930 ******************** regressed\n",
      "[32] 2 torch.float32        0.014,  0.110,  0.119 ******************** regressed\n",
      "[32] 4 torch.float32        0.016,  0.113,  0.122 ******************** regressed\n",
      "[32] 8 torch.float32        0.038,  0.115,  0.123 ******************** regressed\n",
      "[32] 16 torch.float32       0.143,  0.120,  0.158 ******************** regressed\n",
      "[32] 32 torch.float32       0.612,  0.177,  0.194 ******************** regressed\n",
      "[32] 64 torch.float32       2.179,  0.467,  0.354 \n",
      "[32] 128 torch.float32      8.426,  1.213,  1.109 \n",
      "[32] 256 torch.float32      31.850,  2.775,  7.290 ******************** regressed\n",
      "[32] 512 torch.float32      151.800,  8.966,  24.540 ******************** regressed\n",
      "[64] 2 torch.float32        0.018,  0.112,  0.118 ******************** regressed\n",
      "[64] 4 torch.float32        0.022,  0.115,  0.120 ******************** regressed\n",
      "[64] 8 torch.float32        0.068,  0.115,  0.125 ******************** regressed\n",
      "[64] 16 torch.float32       0.273,  0.123,  0.158 ******************** regressed\n",
      "[64] 32 torch.float32       1.434,  0.184,  0.203 ******************** regressed\n",
      "[64] 64 torch.float32       3.854,  0.504,  0.389 \n",
      "[64] 128 torch.float32      16.180,  1.221,  1.489 ******************** regressed\n",
      "[64] 256 torch.float32      66.090,  3.827,  11.370 ******************** regressed\n",
      "[128] 2 torch.float32       0.028,  0.127,  0.161 ******************** regressed\n",
      "[128] 4 torch.float32       0.040,  0.123,  0.154 ******************** regressed\n",
      "[128] 8 torch.float32       0.133,  0.121,  0.154 ******************** regressed\n",
      "[128] 16 torch.float32      0.544,  0.125,  0.189 ******************** regressed\n",
      "[128] 32 torch.float32      2.841,  0.211,  0.252 ******************** regressed\n",
      "[128] 64 torch.float32      6.788,  0.564,  0.541 \n",
      "[128] 128 torch.float32     35.760,  1.744,  2.676 ******************** regressed\n",
      "[256] 2 torch.float32       0.041,  0.117,  0.121 ******************** regressed\n",
      "[256] 4 torch.float32       0.062,  0.118,  0.121 ******************** regressed\n",
      "[256] 8 torch.float32       0.247,  0.124,  0.128 ******************** regressed\n",
      "[256] 16 torch.float32      1.275,  0.135,  0.163 ******************** regressed\n",
      "[256] 32 torch.float32      5.302,  0.246,  0.264 ******************** regressed\n",
      "[256] 64 torch.float32      13.150,  0.854,  0.832 \n",
      "[512] 2 torch.float32       0.072,  0.131,  0.123 \n",
      "[512] 4 torch.float32       0.115,  0.130,  0.123 \n",
      "[512] 8 torch.float32       0.490,  0.134,  0.131 \n",
      "[512] 16 torch.float32      3.102,  0.178,  0.174 \n",
      "[512] 32 torch.float32      10.095,  0.312,  0.365 ******************** regressed\n",
      "[1024] 2 torch.float32      0.134,  0.147,  0.130 \n",
      "[1024] 4 torch.float32      0.223,  0.151,  0.128 \n",
      "[1024] 8 torch.float32      1.093,  0.185,  0.135 \n",
      "[1024] 16 torch.float32     4.458,  0.218,  0.206 \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def readfile(fn):\n",
    "    with open(fn, 'r') as f:\n",
    "        fl = f.readlines()\n",
    "    \n",
    "    dc = {}\n",
    "    dg = {}\n",
    "    for _line in fl:\n",
    "        key, cpu_time, gpu_time = re.split(';|,', _line.rstrip())\n",
    "        dc[key] = float(cpu_time)\n",
    "        dg[key] = float(gpu_time)\n",
    "    \n",
    "    return (dc, dg)\n",
    "\n",
    "def compare():\n",
    "    print('shape'.ljust(26), 'cpu_time, gpu_time_before (magma), gpu_time_after (cusolver/cublas)')\n",
    "    dc_b, dg_b = readfile('before.txt')\n",
    "    dc_a, dg_a = readfile('after.txt')\n",
    "    \n",
    "    for key in dc_b:\n",
    "        cpu_time = 0.5 * (dc_b[key] + dc_a[key])\n",
    "        gpu_time_before = dg_b[key]\n",
    "        gpu_time_after = dg_a[key]\n",
    "        \n",
    "        if gpu_time_after > gpu_time_before:\n",
    "            gs = '*' * 20 + ' regressed'\n",
    "        else:\n",
    "            gs = ''\n",
    "\n",
    "        print(f'{key: <26} {cpu_time: .3f}, {gpu_time_before: .3f}, {gpu_time_after: .3f} {gs}')\n",
    "\n",
    "compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitfce950e88ea94256bae6c6f663f53e68"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
