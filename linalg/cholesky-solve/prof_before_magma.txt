
1.9.0a0+git80a4a50

batch_size, matrix_size, dtype     cpu_time(us), gpu_time(us)
[] 2 torch.float32                  105.062   538.999
[] 4 torch.float32                  9.829   643.682
[] 8 torch.float32                  10.200   557.100
[] 16 torch.float32                 14.926   555.464
[] 32 torch.float32                 17.776   642.091
[] 64 torch.float32                 69.903   571.818
[] 128 torch.float32                114.424   606.802
[] 256 torch.float32                315.489   688.130
[] 512 torch.float32                1381.732   898.367
[] 1024 torch.float32               6764.477   1835.345
[] 2048 torch.float32               44496.086   5850.872
[1] 2 torch.float32                 10.252   48.726
[1] 4 torch.float32                 10.006   48.643
[1] 8 torch.float32                 10.576   49.222
[1] 16 torch.float32                11.061   50.737
[1] 32 torch.float32                14.044   420.964
[1] 64 torch.float32                64.653   401.120
[1] 128 torch.float32               145.727   541.776
[1] 256 torch.float32               331.353   759.410
[1] 512 torch.float32               793.633   1299.163
[1] 1024 torch.float32              3435.338   2696.573
numerical mismatch: reconstruct value compare
With rtol=0.001 and atol=0.001, found 1 element(s) (out of 4194304) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.0012008510529994965 (0.058868251740932465 vs. 0.05766740068793297), which occurred at index (0, 450, 205).
[1] 2048 torch.float32              16042.378   8113.023
[2] 2 torch.float32                 10.456   48.089
[2] 4 torch.float32                 12.554   49.834
[2] 8 torch.float32                 13.926   50.557
[2] 16 torch.float32                11.995   50.617
[2] 32 torch.float32                17.835   341.778
[2] 64 torch.float32                107.280   398.796
[2] 128 torch.float32               264.066   526.272
[2] 256 torch.float32               332.557   794.361
[2] 512 torch.float32               1034.510   1453.463
[2] 1024 torch.float32              6122.146   3387.188
numerical mismatch: reconstruct value compare
With rtol=0.001 and atol=0.001, found 1 element(s) (out of 8388608) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.0010547935962677002 (0.04455813765525818 vs. 0.04350334405899048), which occurred at index (1, 452, 1011).
[2] 2048 torch.float32              44751.598   12493.780
[4] 2 torch.float32                 11.564   48.972
[4] 4 torch.float32                 12.554   49.685
[4] 8 torch.float32                 12.710   50.280
[4] 16 torch.float32                15.370   51.428
[4] 32 torch.float32                26.619   340.295
[4] 64 torch.float32                190.003   401.586
[4] 128 torch.float32               298.653   546.113
[4] 256 torch.float32               586.576   824.993
[4] 512 torch.float32               2146.733   1598.932
[4] 1024 torch.float32              10513.383   4688.041
[4] 2048 torch.float32              81361.482   20951.401
[8] 2 torch.float32                 14.429   49.974
[8] 4 torch.float32                 16.122   50.730
[8] 8 torch.float32                 15.707   51.671
[8] 16 torch.float32                21.012   51.948
[8] 32 torch.float32                42.956   342.143
[8] 64 torch.float32                361.252   407.389
[8] 128 torch.float32               470.324   563.904
[8] 256 torch.float32               768.726   931.009
[8] 512 torch.float32               3572.630   2073.226
[8] 1024 torch.float32              28877.990   7306.103
[16] 2 torch.float32                18.753   48.211
[16] 4 torch.float32                20.763   50.523
[16] 8 torch.float32                21.198   49.508
[16] 16 torch.float32               31.407   51.376
[16] 32 torch.float32               75.767   362.648
[16] 64 torch.float32               618.618   411.162
[16] 128 torch.float32              782.801   581.570
[16] 256 torch.float32              1420.583   1066.276
[16] 512 torch.float32              7972.126   2947.344
[32] 2 torch.float32                27.955   47.266
[32] 4 torch.float32                32.285   50.733
[32] 8 torch.float32                33.263   50.806
[32] 16 torch.float32               53.525   51.630
[32] 32 torch.float32               169.145   355.963
[32] 64 torch.float32               1063.796   421.413
[32] 128 torch.float32              1360.440   660.082
[32] 256 torch.float32              2803.988   1371.614
[64] 2 torch.float32                45.931   50.532
[64] 4 torch.float32                55.463   50.590
[64] 8 torch.float32                57.665   50.521
[64] 16 torch.float32               97.049   51.179
[64] 32 torch.float32               284.588   346.215
[64] 64 torch.float32               1936.029   444.317
[64] 128 torch.float32              2551.218   749.820
[128] 2 torch.float32               82.027   51.022
[128] 4 torch.float32               102.100   51.053
[128] 8 torch.float32               103.924   52.571
[128] 16 torch.float32              183.892   51.708
[128] 32 torch.float32              526.431   356.721
[128] 64 torch.float32              3714.440   478.230
[256] 2 torch.float32               154.366   52.301
[256] 4 torch.float32               194.290   53.303
[256] 8 torch.float32               199.044   53.767
[256] 16 torch.float32              358.212   55.767
[256] 32 torch.float32              940.477   383.397
[512] 2 torch.float32               301.412   55.487
[512] 4 torch.float32               379.328   56.043
[512] 8 torch.float32               391.656   59.123
[512] 16 torch.float32              660.954   61.008
[1024] 2 torch.float32              591.483   66.358
[1024] 4 torch.float32              746.133   65.719
[1024] 8 torch.float32              785.947   68.841
