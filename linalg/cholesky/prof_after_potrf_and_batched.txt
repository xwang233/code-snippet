
1.9.0a0+gitdce610d

batch_size, matrix_size, dtype     cpu_time(us), gpu_time(us)
[] 2 torch.float32                  19.561   35.030
[] 4 torch.float32                  14.126   35.046
[] 8 torch.float32                  14.400   35.175
[] 16 torch.float32                 16.481   35.133
[] 32 torch.float32                 19.957   35.418
[] 64 torch.float32                 46.251   42.917
[] 128 torch.float32                165.895   82.173
[] 256 torch.float32                394.301   171.727
[] 512 torch.float32                1534.215   397.378
[] 1024 torch.float32               6276.541   953.525
[] 2048 torch.float32               45995.387   2755.491
[1] 2 torch.float32                 19.432   35.341
[1] 4 torch.float32                 19.653   35.539
[1] 8 torch.float32                 20.102   35.596
[1] 16 torch.float32                21.609   35.927
[1] 32 torch.float32                24.512   35.769
[1] 64 torch.float32                47.913   42.919
[1] 128 torch.float32               206.286   70.264
[1] 256 torch.float32               448.297   146.682
[1] 512 torch.float32               1199.809   337.876
[1] 1024 torch.float32              4701.004   954.473
[1] 2048 torch.float32              28753.697   2754.500
[2] 2 torch.float32                 23.744   59.240
[2] 4 torch.float32                 22.398   59.264
[2] 8 torch.float32                 23.954   59.102
[2] 16 torch.float32                25.625   59.221
[2] 32 torch.float32                31.775   67.042
[2] 64 torch.float32                90.628   82.712
[2] 128 torch.float32               260.835   111.827
[2] 256 torch.float32               620.326   205.888
[2] 512 torch.float32               1695.673   522.455
[2] 1024 torch.float32              5677.974   1589.960
[2] 2048 torch.float32              41348.952   5928.248
[4] 2 torch.float32                 22.013   42.454
[4] 4 torch.float32                 21.701   42.386
[4] 8 torch.float32                 23.408   42.989
[4] 16 torch.float32                29.048   42.655
[4] 32 torch.float32                42.573   50.501
[4] 64 torch.float32                137.980   63.519
[4] 128 torch.float32               583.711   95.640
[4] 256 torch.float32               941.325   221.888
[4] 512 torch.float32               2626.420   613.503
[4] 1024 torch.float32              6431.612   2107.344
[4] 2048 torch.float32              53383.222   8907.055
[8] 2 torch.float32                 20.810   42.939
[8] 4 torch.float32                 20.216   42.443
[8] 8 torch.float32                 23.398   42.948
[8] 16 torch.float32                32.382   42.498
[8] 32 torch.float32                78.549   51.111
[8] 64 torch.float32                173.434   63.341
[8] 128 torch.float32               762.062   103.029
[8] 256 torch.float32               1245.328   258.526
[8] 512 torch.float32               3772.589   790.236
[8] 1024 torch.float32              14260.092   3151.025
[16] 2 torch.float32                20.788   43.010
[16] 4 torch.float32                22.048   42.318
[16] 8 torch.float32                26.025   42.324
[16] 16 torch.float32               43.716   42.846
[16] 32 torch.float32               124.954   49.931
[16] 64 torch.float32               344.919   63.181
[16] 128 torch.float32              1295.136   117.122
non-determinism: cholesky value output
With rtol=0 and atol=0, found 1 element(s) (out of 1048576) whose difference(s) exceeded the margin of error (including 1 nan comparisons). The greatest difference was nan (nan vs. nan), which occurred at index (2, 255, 255).
numerical mismatch: cholesky value compare
With rtol=0.001 and atol=0.001, found 511 element(s) (out of 1048576) whose difference(s) exceeded the margin of error (including 511 nan comparisons). The greatest difference was nan (-10.53102970123291 vs. nan), which occurred at index (2, 0, 255).
[16] 256 torch.float32              2353.216   335.315
[16] 512 torch.float32              6744.184   1171.981
[32] 2 torch.float32                24.896   42.502
[32] 4 torch.float32                25.302   42.658
[32] 8 torch.float32                33.897   42.715
[32] 16 torch.float32               124.384   43.057
[32] 32 torch.float32               142.719   50.708
[32] 64 torch.float32               540.652   62.631
[32] 128 torch.float32              3011.916   153.772
[32] 256 torch.float32              4714.671   518.030
[64] 2 torch.float32                31.686   43.020
[64] 4 torch.float32                40.574   42.179
[64] 8 torch.float32                52.168   43.572
[64] 16 torch.float32               155.838   42.832
[64] 32 torch.float32               232.285   49.696
[64] 64 torch.float32               946.857   76.027
[64] 128 torch.float32              5422.815   225.720
[128] 2 torch.float32               50.807   42.747
[128] 4 torch.float32               63.164   43.114
[128] 8 torch.float32               95.083   42.930
[128] 16 torch.float32              223.805   42.766
[128] 32 torch.float32              385.848   49.907
[128] 64 torch.float32              1818.460   114.067
[256] 2 torch.float32               64.263   42.242
[256] 4 torch.float32               81.137   42.391
[256] 8 torch.float32               148.079   42.688
[256] 16 torch.float32              337.741   42.174
[256] 32 torch.float32              600.075   64.845
[512] 2 torch.float32               107.343   42.259
[512] 4 torch.float32               135.251   42.090
[512] 8 torch.float32               264.494   42.298
[512] 16 torch.float32              566.473   42.053
[1024] 2 torch.float32              183.681   41.716
[1024] 4 torch.float32              238.899   41.807
[1024] 8 torch.float32              415.170   41.791
