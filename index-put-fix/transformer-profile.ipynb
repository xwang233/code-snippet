{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "\"\"\"\n",
    "Sequence-to-Sequence Modeling with nn.Transformer and TorchText\n",
    "===============================================================\n",
    "\n",
    "This is a tutorial on how to train a sequence-to-sequence model\n",
    "that uses the\n",
    "`nn.Transformer <https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer>`__ module.\n",
    "\n",
    "PyTorch 1.2 release includes a standard transformer module based on the\n",
    "paper `Attention is All You\n",
    "Need <https://arxiv.org/pdf/1706.03762.pdf>`__. The transformer model\n",
    "has been proved to be superior in quality for many sequence-to-sequence\n",
    "problems while being more parallelizable. The ``nn.Transformer`` module\n",
    "relies entirely on an attention mechanism (another module recently\n",
    "implemented as `nn.MultiheadAttention <https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention>`__) to draw global dependencies\n",
    "between input and output. The ``nn.Transformer`` module is now highly\n",
    "modularized such that a single component (like `nn.TransformerEncoder <https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder>`__\n",
    "in this tutorial) can be easily adapted/composed.\n",
    "\n",
    ".. image:: ../_static/img/transformer_architecture.jpg\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "######################################################################\n",
    "# Define the model\n",
    "# ----------------\n",
    "#\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# In this tutorial, we train ``nn.TransformerEncoder`` model on a\n",
    "# language modeling task. The language modeling task is to assign a\n",
    "# probability for the likelihood of a given word (or a sequence of words)\n",
    "# to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
    "# layer first, followed by a positional encoding layer to account for the order\n",
    "# of the word (see the next paragraph for more details). The\n",
    "# ``nn.TransformerEncoder`` consists of multiple layers of\n",
    "# `nn.TransformerEncoderLayer <https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer>`__. Along with the input sequence, a square\n",
    "# attention mask is required because the self-attention layers in\n",
    "# ``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
    "# the sequence. For the language modeling task, any tokens on the future\n",
    "# positions should be masked. To have the actual words, the output\n",
    "# of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
    "# layer, which is followed by a log-Softmax function.\n",
    "#\n",
    "\n",
    "def main(bs):\n",
    "    import math\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    print(torch.__version__)\n",
    "    print(\"batch_size =\", bs)\n",
    "\n",
    "    class TransformerModel(nn.Module):\n",
    "\n",
    "        def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "            super(TransformerModel, self).__init__()\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "            self.model_type = 'Transformer'\n",
    "            self.src_mask = None\n",
    "            self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "            encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "            self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "            self.encoder = nn.Embedding(ntoken, ninp)\n",
    "            self.ninp = ninp\n",
    "            self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "            self.init_weights()\n",
    "\n",
    "        def _generate_square_subsequent_mask(self, sz):\n",
    "            mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "            mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "            return mask\n",
    "\n",
    "        def init_weights(self):\n",
    "            initrange = 0.1\n",
    "            self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "            self.decoder.bias.data.zero_()\n",
    "            self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "        def forward(self, src):\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                device = src.device\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "\n",
    "            src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "            src = self.pos_encoder(src)\n",
    "            output = self.transformer_encoder(src, self.src_mask)\n",
    "            output = self.decoder(output)\n",
    "            return output\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # ``PositionalEncoding`` module injects some information about the\n",
    "    # relative or absolute position of the tokens in the sequence. The\n",
    "    # positional encodings have the same dimension as the embeddings so that\n",
    "    # the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
    "    # different frequencies.\n",
    "    #\n",
    "\n",
    "    class PositionalEncoding(nn.Module):\n",
    "\n",
    "        def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "            super(PositionalEncoding, self).__init__()\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "            pe = torch.zeros(max_len, d_model)\n",
    "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "            pe[:, 0::2] = torch.sin(position * div_term)\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "            pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "            self.register_buffer('pe', pe)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x + self.pe[:x.size(0), :]\n",
    "            return self.dropout(x)\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # Load and batch data\n",
    "    # -------------------\n",
    "    #\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # The training process uses Wikitext-2 dataset from ``torchtext``. The\n",
    "    # vocab object is built based on the train dataset and is used to numericalize\n",
    "    # tokens into tensors. Starting from sequential data, the ``batchify()``\n",
    "    # function arranges the dataset into columns, trimming off any tokens remaining\n",
    "    # after the data has been divided into batches of size ``batch_size``.\n",
    "    # For instance, with the alphabet as the sequence (total length of 26)\n",
    "    # and a batch size of 4, we would divide the alphabet into 4 sequences of\n",
    "    # length 6:\n",
    "    #\n",
    "    # .. math::\n",
    "    #   \\begin{bmatrix}\n",
    "    #   \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
    "    #   \\end{bmatrix}\n",
    "    #   \\Rightarrow\n",
    "    #   \\begin{bmatrix}\n",
    "    #   \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
    "    #   \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
    "    #   \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
    "    #   \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
    "    #   \\end{bmatrix}\n",
    "    #\n",
    "    # These columns are treated as independent by the model, which means that\n",
    "    # the dependence of ``G`` and ``F`` can not be learned, but allows more\n",
    "    # efficient batch processing.\n",
    "    #\n",
    "\n",
    "    import torchtext\n",
    "    from torchtext.data.utils import get_tokenizer\n",
    "    TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                                init_token='<sos>',\n",
    "                                eos_token='<eos>',\n",
    "                                lower=True)\n",
    "    train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "    TEXT.build_vocab(train_txt)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def batchify(data, bsz):\n",
    "        data = TEXT.numericalize([data.examples[0].text])\n",
    "        # Divide the dataset into bsz parts.\n",
    "        nbatch = data.size(0) // bsz\n",
    "        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "        data = data.narrow(0, 0, nbatch * bsz)\n",
    "        # Evenly divide the data across the bsz batches.\n",
    "        data = data.view(bsz, -1).t().contiguous()\n",
    "        return data.to(device)\n",
    "\n",
    "    # batch_size = 320\n",
    "    batch_size = bs\n",
    "    eval_batch_size = 10\n",
    "    train_data = batchify(train_txt, batch_size)\n",
    "    val_data = batchify(val_txt, eval_batch_size)\n",
    "    test_data = batchify(test_txt, eval_batch_size)\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # Functions to generate input and target sequence\n",
    "    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # ``get_batch()`` function generates the input and target sequence for\n",
    "    # the transformer model. It subdivides the source data into chunks of\n",
    "    # length ``bptt``. For the language modeling task, the model needs the\n",
    "    # following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
    "    # we’d get the following two Variables for ``i`` = 0:\n",
    "    #\n",
    "    # .. image:: ../_static/img/transformer_input_target.png\n",
    "    #\n",
    "    # It should be noted that the chunks are along dimension 0, consistent\n",
    "    # with the ``S`` dimension in the Transformer model. The batch dimension\n",
    "    # ``N`` is along dimension 1.\n",
    "    #\n",
    "\n",
    "    bptt = 35\n",
    "    def get_batch(source, i):\n",
    "        seq_len = min(bptt, len(source) - 1 - i)\n",
    "        data = source[i:i+seq_len]\n",
    "        target = source[i+1:i+1+seq_len].view(-1)\n",
    "        return data, target\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # Initiate an instance\n",
    "    # --------------------\n",
    "    #\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # The model is set up with the hyperparameter below. The vocab size is\n",
    "    # equal to the length of the vocab object.\n",
    "    #\n",
    "\n",
    "    ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "    emsize = 200 # embedding dimension\n",
    "    nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "    nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead = 2 # the number of heads in the multiheadattention models\n",
    "    dropout = 0.2 # the dropout value\n",
    "    model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # Run the model\n",
    "    # -------------\n",
    "    #\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # `CrossEntropyLoss <https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\n",
    "    # is applied to track the loss and\n",
    "    # `SGD <https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD>`__\n",
    "    # implements stochastic gradient descent method as the optimizer. The initial\n",
    "    # learning rate is set to 5.0. `StepLR <https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR>`__ is\n",
    "    # applied to adjust the learn rate through epochs. During the\n",
    "    # training, we use\n",
    "    # `nn.utils.clip_grad_norm\\_ <https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_>`__\n",
    "    # function to scale all the gradient together to prevent exploding.\n",
    "    #\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 5.0 # learning rate\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "    import time\n",
    "    def train():\n",
    "        model.train() # Turn on the train mode\n",
    "        total_loss = 0.\n",
    "        start_time = time.time()\n",
    "        ntokens = len(TEXT.vocab.stoi)\n",
    "        for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "            data, targets = get_batch(train_data, i)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output.view(-1, ntokens), targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            log_interval = 200 * 20 // batch_size\n",
    "            if batch % log_interval == 0 and batch > 0:\n",
    "                cur_loss = total_loss / log_interval\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                      'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                      ' ms/sample {:7.5f} | '\n",
    "                      'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                        epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                        elapsed * 1000 / log_interval,\n",
    "                        elapsed * 1000 / log_interval / batch_size,\n",
    "                        cur_loss, math.exp(cur_loss)))\n",
    "                total_loss = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "    def evaluate(eval_model, data_source):\n",
    "        eval_model.eval() # Turn on the evaluation mode\n",
    "        total_loss = 0.\n",
    "        ntokens = len(TEXT.vocab.stoi)\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, data_source.size(0) - 1, bptt):\n",
    "                data, targets = get_batch(data_source, i)\n",
    "                output = eval_model(data)\n",
    "                output_flat = output.view(-1, ntokens)\n",
    "                total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "        return total_loss / (len(data_source) - 1)\n",
    "\n",
    "    ######################################################################\n",
    "    # Loop over epochs. Save the model if the validation loss is the best\n",
    "    # we've seen so far. Adjust the learning rate after each epoch.\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs = 3 # The number of epochs\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                         val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # Evaluate the model with the test dataset\n",
    "    # -------------------------------------\n",
    "    #\n",
    "    # Apply the best model to check the result with the test dataset.\n",
    "\n",
    "    test_loss = evaluate(best_model, test_data)\n",
    "    print('=' * 89)\n",
    "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "        test_loss, math.exp(test_loss)))\n",
    "    print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0a0+2f8ed14\n",
      "batch_size = 40\n",
      "| epoch   1 |   100/ 1490 batches | lr 5.00 | ms/batch 18.96 |  ms/sample 0.47389 | loss  8.68 | ppl  5867.16\n",
      "| epoch   1 |   200/ 1490 batches | lr 5.00 | ms/batch 18.69 |  ms/sample 0.46713 | loss  7.25 | ppl  1414.22\n",
      "| epoch   1 |   300/ 1490 batches | lr 5.00 | ms/batch 18.05 |  ms/sample 0.45113 | loss  6.86 | ppl   951.20\n",
      "| epoch   1 |   400/ 1490 batches | lr 5.00 | ms/batch 18.73 |  ms/sample 0.46815 | loss  6.61 | ppl   738.82\n",
      "| epoch   1 |   500/ 1490 batches | lr 5.00 | ms/batch 12.59 |  ms/sample 0.31475 | loss  6.43 | ppl   619.26\n",
      "| epoch   1 |   600/ 1490 batches | lr 5.00 | ms/batch 18.38 |  ms/sample 0.45949 | loss  6.29 | ppl   537.15\n",
      "| epoch   1 |   700/ 1490 batches | lr 5.00 | ms/batch 19.77 |  ms/sample 0.49428 | loss  6.19 | ppl   486.55\n",
      "| epoch   1 |   800/ 1490 batches | lr 5.00 | ms/batch 19.05 |  ms/sample 0.47614 | loss  6.12 | ppl   454.45\n",
      "| epoch   1 |   900/ 1490 batches | lr 5.00 | ms/batch 19.10 |  ms/sample 0.47757 | loss  6.09 | ppl   442.17\n",
      "| epoch   1 |  1000/ 1490 batches | lr 5.00 | ms/batch 17.23 |  ms/sample 0.43087 | loss  6.04 | ppl   421.32\n",
      "| epoch   1 |  1100/ 1490 batches | lr 5.00 | ms/batch 16.87 |  ms/sample 0.42181 | loss  6.03 | ppl   413.78\n",
      "| epoch   1 |  1200/ 1490 batches | lr 5.00 | ms/batch 19.06 |  ms/sample 0.47645 | loss  5.97 | ppl   389.63\n",
      "| epoch   1 |  1300/ 1490 batches | lr 5.00 | ms/batch 18.88 |  ms/sample 0.47204 | loss  5.96 | ppl   386.06\n",
      "| epoch   1 |  1400/ 1490 batches | lr 5.00 | ms/batch 19.06 |  ms/sample 0.47659 | loss  5.88 | ppl   358.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 28.40s | valid loss  5.71 | valid ppl   301.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   100/ 1490 batches | lr 4.51 | ms/batch 18.97 |  ms/sample 0.47421 | loss  5.91 | ppl   368.73\n",
      "| epoch   2 |   200/ 1490 batches | lr 4.51 | ms/batch 19.13 |  ms/sample 0.47829 | loss  5.78 | ppl   324.79\n",
      "| epoch   2 |   300/ 1490 batches | lr 4.51 | ms/batch 19.01 |  ms/sample 0.47521 | loss  5.78 | ppl   322.62\n",
      "| epoch   2 |   400/ 1490 batches | lr 4.51 | ms/batch 18.54 |  ms/sample 0.46356 | loss  5.78 | ppl   324.09\n",
      "| epoch   2 |   500/ 1490 batches | lr 4.51 | ms/batch 19.29 |  ms/sample 0.48229 | loss  5.68 | ppl   292.98\n",
      "| epoch   2 |   600/ 1490 batches | lr 4.51 | ms/batch 18.02 |  ms/sample 0.45060 | loss  5.61 | ppl   273.70\n",
      "| epoch   2 |   700/ 1490 batches | lr 4.51 | ms/batch 16.02 |  ms/sample 0.40049 | loss  5.60 | ppl   269.65\n",
      "| epoch   2 |   800/ 1490 batches | lr 4.51 | ms/batch 12.55 |  ms/sample 0.31372 | loss  5.60 | ppl   269.99\n",
      "| epoch   2 |   900/ 1490 batches | lr 4.51 | ms/batch 13.36 |  ms/sample 0.33411 | loss  5.63 | ppl   277.56\n",
      "| epoch   2 |  1000/ 1490 batches | lr 4.51 | ms/batch 18.73 |  ms/sample 0.46833 | loss  5.57 | ppl   263.37\n",
      "| epoch   2 |  1100/ 1490 batches | lr 4.51 | ms/batch 18.93 |  ms/sample 0.47322 | loss  5.62 | ppl   275.22\n",
      "| epoch   2 |  1200/ 1490 batches | lr 4.51 | ms/batch 19.11 |  ms/sample 0.47772 | loss  5.56 | ppl   260.74\n",
      "| epoch   2 |  1300/ 1490 batches | lr 4.51 | ms/batch 19.72 |  ms/sample 0.49302 | loss  5.59 | ppl   267.13\n",
      "| epoch   2 |  1400/ 1490 batches | lr 4.51 | ms/batch 18.65 |  ms/sample 0.46631 | loss  5.51 | ppl   247.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 28.02s | valid loss  5.52 | valid ppl   250.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   100/ 1490 batches | lr 4.29 | ms/batch 15.14 |  ms/sample 0.37844 | loss  5.61 | ppl   272.47\n",
      "| epoch   3 |   200/ 1490 batches | lr 4.29 | ms/batch 12.68 |  ms/sample 0.31701 | loss  5.47 | ppl   237.89\n",
      "| epoch   3 |   300/ 1490 batches | lr 4.29 | ms/batch 12.50 |  ms/sample 0.31241 | loss  5.51 | ppl   247.00\n",
      "| epoch   3 |   400/ 1490 batches | lr 4.29 | ms/batch 15.01 |  ms/sample 0.37513 | loss  5.53 | ppl   250.99\n",
      "| epoch   3 |   500/ 1490 batches | lr 4.29 | ms/batch 19.18 |  ms/sample 0.47962 | loss  5.42 | ppl   225.29\n",
      "| epoch   3 |   600/ 1490 batches | lr 4.29 | ms/batch 19.13 |  ms/sample 0.47822 | loss  5.34 | ppl   209.09\n",
      "| epoch   3 |   700/ 1490 batches | lr 4.29 | ms/batch 19.20 |  ms/sample 0.47992 | loss  5.34 | ppl   208.22\n",
      "| epoch   3 |   800/ 1490 batches | lr 4.29 | ms/batch 16.11 |  ms/sample 0.40284 | loss  5.35 | ppl   211.49\n",
      "| epoch   3 |   900/ 1490 batches | lr 4.29 | ms/batch 12.52 |  ms/sample 0.31309 | loss  5.40 | ppl   220.54\n",
      "| epoch   3 |  1000/ 1490 batches | lr 4.29 | ms/batch 12.60 |  ms/sample 0.31492 | loss  5.34 | ppl   208.96\n",
      "| epoch   3 |  1100/ 1490 batches | lr 4.29 | ms/batch 12.61 |  ms/sample 0.31523 | loss  5.39 | ppl   218.84\n",
      "| epoch   3 |  1200/ 1490 batches | lr 4.29 | ms/batch 12.64 |  ms/sample 0.31606 | loss  5.35 | ppl   210.58\n",
      "| epoch   3 |  1300/ 1490 batches | lr 4.29 | ms/batch 12.58 |  ms/sample 0.31452 | loss  5.38 | ppl   217.87\n",
      "| epoch   3 |  1400/ 1490 batches | lr 4.29 | ms/batch 12.57 |  ms/sample 0.31431 | loss  5.30 | ppl   201.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 23.43s | valid loss  5.40 | valid ppl   220.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.31 | test ppl   203.29\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "main(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0a0+2f8ed14\n",
      "batch_size = 320\n",
      "| epoch   1 |    12/  186 batches | lr 5.00 | ms/batch 81.22 |  ms/sample 0.25382 | loss 11.02 | ppl 61237.93\n",
      "| epoch   1 |    24/  186 batches | lr 5.00 | ms/batch 74.28 |  ms/sample 0.23213 | loss  8.82 | ppl  6760.98\n",
      "| epoch   1 |    36/  186 batches | lr 5.00 | ms/batch 74.26 |  ms/sample 0.23206 | loss  9.21 | ppl  9960.32\n",
      "| epoch   1 |    48/  186 batches | lr 5.00 | ms/batch 74.42 |  ms/sample 0.23255 | loss  8.32 | ppl  4102.25\n",
      "| epoch   1 |    60/  186 batches | lr 5.00 | ms/batch 74.20 |  ms/sample 0.23187 | loss  8.33 | ppl  4164.04\n",
      "| epoch   1 |    72/  186 batches | lr 5.00 | ms/batch 73.83 |  ms/sample 0.23073 | loss  8.04 | ppl  3087.15\n",
      "| epoch   1 |    84/  186 batches | lr 5.00 | ms/batch 74.21 |  ms/sample 0.23192 | loss  7.87 | ppl  2628.42\n",
      "| epoch   1 |    96/  186 batches | lr 5.00 | ms/batch 74.09 |  ms/sample 0.23154 | loss  7.87 | ppl  2622.24\n",
      "| epoch   1 |   108/  186 batches | lr 5.00 | ms/batch 74.06 |  ms/sample 0.23143 | loss  7.77 | ppl  2370.40\n",
      "| epoch   1 |   120/  186 batches | lr 5.00 | ms/batch 74.11 |  ms/sample 0.23159 | loss  7.64 | ppl  2074.16\n",
      "| epoch   1 |   132/  186 batches | lr 5.00 | ms/batch 74.82 |  ms/sample 0.23380 | loss  7.52 | ppl  1847.30\n",
      "| epoch   1 |   144/  186 batches | lr 5.00 | ms/batch 78.39 |  ms/sample 0.24498 | loss  7.48 | ppl  1776.96\n",
      "| epoch   1 |   156/  186 batches | lr 5.00 | ms/batch 78.57 |  ms/sample 0.24553 | loss  7.39 | ppl  1617.39\n",
      "| epoch   1 |   168/  186 batches | lr 5.00 | ms/batch 78.21 |  ms/sample 0.24439 | loss  7.25 | ppl  1410.30\n",
      "| epoch   1 |   180/  186 batches | lr 5.00 | ms/batch 78.34 |  ms/sample 0.24482 | loss  7.25 | ppl  1410.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 15.31s | valid loss  6.74 | valid ppl   846.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    12/  186 batches | lr 4.51 | ms/batch 80.11 |  ms/sample 0.25036 | loss  7.60 | ppl  1998.30\n",
      "| epoch   2 |    24/  186 batches | lr 4.51 | ms/batch 76.23 |  ms/sample 0.23821 | loss  6.99 | ppl  1089.18\n",
      "| epoch   2 |    36/  186 batches | lr 4.51 | ms/batch 76.82 |  ms/sample 0.24007 | loss  6.88 | ppl   975.57\n",
      "| epoch   2 |    48/  186 batches | lr 4.51 | ms/batch 73.94 |  ms/sample 0.23106 | loss  6.87 | ppl   963.81\n",
      "| epoch   2 |    60/  186 batches | lr 4.51 | ms/batch 74.26 |  ms/sample 0.23206 | loss  6.87 | ppl   965.52\n",
      "| epoch   2 |    72/  186 batches | lr 4.51 | ms/batch 74.15 |  ms/sample 0.23172 | loss  6.79 | ppl   888.67\n",
      "| epoch   2 |    84/  186 batches | lr 4.51 | ms/batch 74.12 |  ms/sample 0.23163 | loss  6.79 | ppl   887.88\n",
      "| epoch   2 |    96/  186 batches | lr 4.51 | ms/batch 74.18 |  ms/sample 0.23182 | loss  6.80 | ppl   898.64\n",
      "| epoch   2 |   108/  186 batches | lr 4.51 | ms/batch 73.99 |  ms/sample 0.23121 | loss  6.72 | ppl   830.45\n",
      "| epoch   2 |   120/  186 batches | lr 4.51 | ms/batch 73.97 |  ms/sample 0.23115 | loss  6.68 | ppl   797.91\n",
      "| epoch   2 |   132/  186 batches | lr 4.51 | ms/batch 73.95 |  ms/sample 0.23109 | loss  6.68 | ppl   793.57\n",
      "| epoch   2 |   144/  186 batches | lr 4.51 | ms/batch 73.97 |  ms/sample 0.23114 | loss  6.57 | ppl   715.85\n",
      "| epoch   2 |   156/  186 batches | lr 4.51 | ms/batch 73.85 |  ms/sample 0.23078 | loss  6.57 | ppl   711.16\n",
      "| epoch   2 |   168/  186 batches | lr 4.51 | ms/batch 74.03 |  ms/sample 0.23133 | loss  6.62 | ppl   747.88\n",
      "| epoch   2 |   180/  186 batches | lr 4.51 | ms/batch 73.95 |  ms/sample 0.23108 | loss  6.60 | ppl   736.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 15.07s | valid loss  6.39 | valid ppl   594.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    12/  186 batches | lr 4.29 | ms/batch 80.00 |  ms/sample 0.25000 | loss  6.99 | ppl  1081.05\n",
      "| epoch   3 |    24/  186 batches | lr 4.29 | ms/batch 73.90 |  ms/sample 0.23094 | loss  6.46 | ppl   638.55\n",
      "| epoch   3 |    36/  186 batches | lr 4.29 | ms/batch 74.10 |  ms/sample 0.23155 | loss  6.40 | ppl   601.52\n",
      "| epoch   3 |    48/  186 batches | lr 4.29 | ms/batch 73.96 |  ms/sample 0.23112 | loss  6.42 | ppl   614.77\n",
      "| epoch   3 |    60/  186 batches | lr 4.29 | ms/batch 73.97 |  ms/sample 0.23117 | loss  6.39 | ppl   596.65\n",
      "| epoch   3 |    72/  186 batches | lr 4.29 | ms/batch 74.04 |  ms/sample 0.23137 | loss  6.39 | ppl   596.18\n",
      "| epoch   3 |    84/  186 batches | lr 4.29 | ms/batch 75.57 |  ms/sample 0.23617 | loss  6.36 | ppl   578.31\n",
      "| epoch   3 |    96/  186 batches | lr 4.29 | ms/batch 77.80 |  ms/sample 0.24314 | loss  6.32 | ppl   555.62\n",
      "| epoch   3 |   108/  186 batches | lr 4.29 | ms/batch 78.07 |  ms/sample 0.24398 | loss  6.33 | ppl   559.99\n",
      "| epoch   3 |   120/  186 batches | lr 4.29 | ms/batch 78.10 |  ms/sample 0.24407 | loss  6.28 | ppl   533.30\n",
      "| epoch   3 |   132/  186 batches | lr 4.29 | ms/batch 77.92 |  ms/sample 0.24349 | loss  6.29 | ppl   541.75\n",
      "| epoch   3 |   144/  186 batches | lr 4.29 | ms/batch 76.77 |  ms/sample 0.23992 | loss  6.27 | ppl   526.66\n",
      "| epoch   3 |   156/  186 batches | lr 4.29 | ms/batch 74.04 |  ms/sample 0.23139 | loss  6.30 | ppl   543.75\n",
      "| epoch   3 |   168/  186 batches | lr 4.29 | ms/batch 73.96 |  ms/sample 0.23113 | loss  6.24 | ppl   512.71\n",
      "| epoch   3 |   180/  186 batches | lr 4.29 | ms/batch 73.91 |  ms/sample 0.23097 | loss  6.16 | ppl   474.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 15.28s | valid loss  6.00 | valid ppl   403.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.93 | test ppl   376.74\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "main(320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0a0+4460c8b\n",
      "batch_size = 40\n",
      "| epoch   1 |   100/ 1490 batches | lr 5.00 | ms/batch 18.51 |  ms/sample 0.46284 | loss  8.77 | ppl  6448.17\n",
      "| epoch   1 |   200/ 1490 batches | lr 5.00 | ms/batch 18.53 |  ms/sample 0.46323 | loss  7.37 | ppl  1581.23\n",
      "| epoch   1 |   300/ 1490 batches | lr 5.00 | ms/batch 18.48 |  ms/sample 0.46189 | loss  6.91 | ppl  1007.15\n",
      "| epoch   1 |   400/ 1490 batches | lr 5.00 | ms/batch 18.85 |  ms/sample 0.47132 | loss  6.67 | ppl   789.62\n",
      "| epoch   1 |   500/ 1490 batches | lr 5.00 | ms/batch 18.70 |  ms/sample 0.46758 | loss  6.47 | ppl   646.16\n",
      "| epoch   1 |   600/ 1490 batches | lr 5.00 | ms/batch 18.49 |  ms/sample 0.46216 | loss  6.31 | ppl   552.36\n",
      "| epoch   1 |   700/ 1490 batches | lr 5.00 | ms/batch 18.69 |  ms/sample 0.46725 | loss  6.21 | ppl   497.01\n",
      "| epoch   1 |   800/ 1490 batches | lr 5.00 | ms/batch 18.48 |  ms/sample 0.46194 | loss  6.14 | ppl   462.19\n",
      "| epoch   1 |   900/ 1490 batches | lr 5.00 | ms/batch 18.37 |  ms/sample 0.45914 | loss  6.11 | ppl   449.57\n",
      "| epoch   1 |  1000/ 1490 batches | lr 5.00 | ms/batch 18.38 |  ms/sample 0.45946 | loss  6.06 | ppl   429.29\n",
      "| epoch   1 |  1100/ 1490 batches | lr 5.00 | ms/batch 18.38 |  ms/sample 0.45947 | loss  6.05 | ppl   425.29\n",
      "| epoch   1 |  1200/ 1490 batches | lr 5.00 | ms/batch 18.37 |  ms/sample 0.45932 | loss  5.99 | ppl   398.02\n",
      "| epoch   1 |  1300/ 1490 batches | lr 5.00 | ms/batch 14.25 |  ms/sample 0.35636 | loss  5.98 | ppl   396.27\n",
      "| epoch   1 |  1400/ 1490 batches | lr 5.00 | ms/batch 17.85 |  ms/sample 0.44616 | loss  5.91 | ppl   367.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 28.35s | valid loss  5.74 | valid ppl   310.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   100/ 1490 batches | lr 4.51 | ms/batch 18.86 |  ms/sample 0.47151 | loss  5.93 | ppl   377.88\n",
      "| epoch   2 |   200/ 1490 batches | lr 4.51 | ms/batch 18.30 |  ms/sample 0.45738 | loss  5.80 | ppl   330.86\n",
      "| epoch   2 |   300/ 1490 batches | lr 4.51 | ms/batch 16.73 |  ms/sample 0.41827 | loss  5.79 | ppl   328.62\n",
      "| epoch   2 |   400/ 1490 batches | lr 4.51 | ms/batch 18.42 |  ms/sample 0.46053 | loss  5.80 | ppl   329.11\n",
      "| epoch   2 |   500/ 1490 batches | lr 4.51 | ms/batch 18.56 |  ms/sample 0.46408 | loss  5.70 | ppl   298.51\n",
      "| epoch   2 |   600/ 1490 batches | lr 4.51 | ms/batch 18.62 |  ms/sample 0.46560 | loss  5.63 | ppl   278.64\n",
      "| epoch   2 |   700/ 1490 batches | lr 4.51 | ms/batch 18.57 |  ms/sample 0.46428 | loss  5.61 | ppl   274.38\n",
      "| epoch   2 |   800/ 1490 batches | lr 4.51 | ms/batch 18.62 |  ms/sample 0.46559 | loss  5.62 | ppl   274.68\n",
      "| epoch   2 |   900/ 1490 batches | lr 4.51 | ms/batch 17.90 |  ms/sample 0.44761 | loss  5.64 | ppl   280.25\n",
      "| epoch   2 |  1000/ 1490 batches | lr 4.51 | ms/batch 18.87 |  ms/sample 0.47184 | loss  5.59 | ppl   267.18\n",
      "| epoch   2 |  1100/ 1490 batches | lr 4.51 | ms/batch 18.90 |  ms/sample 0.47242 | loss  5.63 | ppl   278.07\n",
      "| epoch   2 |  1200/ 1490 batches | lr 4.51 | ms/batch 18.75 |  ms/sample 0.46869 | loss  5.59 | ppl   267.24\n",
      "| epoch   2 |  1300/ 1490 batches | lr 4.51 | ms/batch 18.60 |  ms/sample 0.46498 | loss  5.61 | ppl   272.08\n",
      "| epoch   2 |  1400/ 1490 batches | lr 4.51 | ms/batch 19.13 |  ms/sample 0.47835 | loss  5.54 | ppl   253.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 28.82s | valid loss  5.51 | valid ppl   246.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   100/ 1490 batches | lr 4.29 | ms/batch 18.64 |  ms/sample 0.46602 | loss  5.62 | ppl   276.17\n",
      "| epoch   3 |   200/ 1490 batches | lr 4.29 | ms/batch 15.25 |  ms/sample 0.38119 | loss  5.49 | ppl   242.59\n",
      "| epoch   3 |   300/ 1490 batches | lr 4.29 | ms/batch 12.64 |  ms/sample 0.31605 | loss  5.53 | ppl   251.46\n",
      "| epoch   3 |   400/ 1490 batches | lr 4.29 | ms/batch 16.76 |  ms/sample 0.41897 | loss  5.54 | ppl   255.42\n",
      "| epoch   3 |   500/ 1490 batches | lr 4.29 | ms/batch 14.19 |  ms/sample 0.35479 | loss  5.43 | ppl   227.35\n",
      "| epoch   3 |   600/ 1490 batches | lr 4.29 | ms/batch 12.67 |  ms/sample 0.31684 | loss  5.36 | ppl   212.59\n",
      "| epoch   3 |   700/ 1490 batches | lr 4.29 | ms/batch 12.68 |  ms/sample 0.31707 | loss  5.35 | ppl   210.11\n",
      "| epoch   3 |   800/ 1490 batches | lr 4.29 | ms/batch 17.53 |  ms/sample 0.43834 | loss  5.37 | ppl   214.55\n",
      "| epoch   3 |   900/ 1490 batches | lr 4.29 | ms/batch 13.05 |  ms/sample 0.32621 | loss  5.41 | ppl   224.33\n",
      "| epoch   3 |  1000/ 1490 batches | lr 4.29 | ms/batch 14.29 |  ms/sample 0.35732 | loss  5.36 | ppl   212.35\n",
      "| epoch   3 |  1100/ 1490 batches | lr 4.29 | ms/batch 13.21 |  ms/sample 0.33024 | loss  5.40 | ppl   222.31\n",
      "| epoch   3 |  1200/ 1490 batches | lr 4.29 | ms/batch 17.90 |  ms/sample 0.44760 | loss  5.36 | ppl   213.30\n",
      "| epoch   3 |  1300/ 1490 batches | lr 4.29 | ms/batch 18.63 |  ms/sample 0.46585 | loss  5.39 | ppl   219.40\n",
      "| epoch   3 |  1400/ 1490 batches | lr 4.29 | ms/batch 17.46 |  ms/sample 0.43654 | loss  5.32 | ppl   204.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 24.03s | valid loss  5.41 | valid ppl   224.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.32 | test ppl   205.40\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "main(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0a0+4460c8b\n",
      "batch_size = 320\n",
      "| epoch   1 |    12/  186 batches | lr 5.00 | ms/batch 87.05 |  ms/sample 0.27205 | loss 11.55 | ppl 104023.84\n",
      "| epoch   1 |    24/  186 batches | lr 5.00 | ms/batch 79.23 |  ms/sample 0.24760 | loss  9.31 | ppl 11073.43\n",
      "| epoch   1 |    36/  186 batches | lr 5.00 | ms/batch 77.99 |  ms/sample 0.24371 | loss  8.76 | ppl  6344.89\n",
      "| epoch   1 |    48/  186 batches | lr 5.00 | ms/batch 74.40 |  ms/sample 0.23249 | loss  8.42 | ppl  4550.88\n",
      "| epoch   1 |    60/  186 batches | lr 5.00 | ms/batch 78.55 |  ms/sample 0.24547 | loss  8.48 | ppl  4794.43\n",
      "| epoch   1 |    72/  186 batches | lr 5.00 | ms/batch 79.57 |  ms/sample 0.24867 | loss  8.10 | ppl  3291.56\n",
      "| epoch   1 |    84/  186 batches | lr 5.00 | ms/batch 79.31 |  ms/sample 0.24783 | loss  8.10 | ppl  3306.62\n",
      "| epoch   1 |    96/  186 batches | lr 5.00 | ms/batch 79.80 |  ms/sample 0.24938 | loss  7.80 | ppl  2434.16\n",
      "| epoch   1 |   108/  186 batches | lr 5.00 | ms/batch 79.21 |  ms/sample 0.24752 | loss  7.61 | ppl  2010.49\n",
      "| epoch   1 |   120/  186 batches | lr 5.00 | ms/batch 79.05 |  ms/sample 0.24702 | loss  7.55 | ppl  1902.27\n",
      "| epoch   1 |   132/  186 batches | lr 5.00 | ms/batch 79.21 |  ms/sample 0.24754 | loss  7.47 | ppl  1751.45\n",
      "| epoch   1 |   144/  186 batches | lr 5.00 | ms/batch 78.47 |  ms/sample 0.24522 | loss  7.37 | ppl  1590.42\n",
      "| epoch   1 |   156/  186 batches | lr 5.00 | ms/batch 78.79 |  ms/sample 0.24622 | loss  7.32 | ppl  1504.88\n",
      "| epoch   1 |   168/  186 batches | lr 5.00 | ms/batch 78.64 |  ms/sample 0.24574 | loss  7.22 | ppl  1366.44\n",
      "| epoch   1 |   180/  186 batches | lr 5.00 | ms/batch 78.46 |  ms/sample 0.24519 | loss  7.21 | ppl  1352.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 15.91s | valid loss  6.74 | valid ppl   846.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    12/  186 batches | lr 4.51 | ms/batch 84.83 |  ms/sample 0.26509 | loss  7.56 | ppl  1915.75\n",
      "| epoch   2 |    24/  186 batches | lr 4.51 | ms/batch 78.50 |  ms/sample 0.24530 | loss  6.98 | ppl  1077.90\n",
      "| epoch   2 |    36/  186 batches | lr 4.51 | ms/batch 78.48 |  ms/sample 0.24526 | loss  6.92 | ppl  1010.62\n",
      "| epoch   2 |    48/  186 batches | lr 4.51 | ms/batch 78.38 |  ms/sample 0.24493 | loss  6.90 | ppl   990.53\n",
      "| epoch   2 |    60/  186 batches | lr 4.51 | ms/batch 78.34 |  ms/sample 0.24482 | loss  6.85 | ppl   944.88\n",
      "| epoch   2 |    72/  186 batches | lr 4.51 | ms/batch 78.31 |  ms/sample 0.24473 | loss  6.74 | ppl   843.47\n",
      "| epoch   2 |    84/  186 batches | lr 4.51 | ms/batch 78.40 |  ms/sample 0.24498 | loss  6.77 | ppl   869.13\n",
      "| epoch   2 |    96/  186 batches | lr 4.51 | ms/batch 78.43 |  ms/sample 0.24510 | loss  6.73 | ppl   840.14\n",
      "| epoch   2 |   108/  186 batches | lr 4.51 | ms/batch 78.75 |  ms/sample 0.24608 | loss  6.73 | ppl   839.17\n",
      "| epoch   2 |   120/  186 batches | lr 4.51 | ms/batch 78.37 |  ms/sample 0.24492 | loss  6.61 | ppl   742.20\n",
      "| epoch   2 |   132/  186 batches | lr 4.51 | ms/batch 78.17 |  ms/sample 0.24428 | loss  6.65 | ppl   774.60\n",
      "| epoch   2 |   144/  186 batches | lr 4.51 | ms/batch 78.22 |  ms/sample 0.24443 | loss  6.61 | ppl   740.59\n",
      "| epoch   2 |   156/  186 batches | lr 4.51 | ms/batch 78.46 |  ms/sample 0.24518 | loss  6.61 | ppl   743.45\n",
      "| epoch   2 |   168/  186 batches | lr 4.51 | ms/batch 78.56 |  ms/sample 0.24550 | loss  6.54 | ppl   691.07\n",
      "| epoch   2 |   180/  186 batches | lr 4.51 | ms/batch 78.32 |  ms/sample 0.24474 | loss  6.54 | ppl   692.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 15.88s | valid loss  6.10 | valid ppl   444.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    12/  186 batches | lr 4.29 | ms/batch 84.73 |  ms/sample 0.26479 | loss  6.94 | ppl  1035.41\n",
      "| epoch   3 |    24/  186 batches | lr 4.29 | ms/batch 78.06 |  ms/sample 0.24393 | loss  6.41 | ppl   607.57\n",
      "| epoch   3 |    36/  186 batches | lr 4.29 | ms/batch 78.34 |  ms/sample 0.24481 | loss  6.40 | ppl   599.14\n",
      "| epoch   3 |    48/  186 batches | lr 4.29 | ms/batch 78.27 |  ms/sample 0.24460 | loss  6.38 | ppl   588.16\n",
      "| epoch   3 |    60/  186 batches | lr 4.29 | ms/batch 78.26 |  ms/sample 0.24456 | loss  6.38 | ppl   588.17\n",
      "| epoch   3 |    72/  186 batches | lr 4.29 | ms/batch 78.91 |  ms/sample 0.24661 | loss  6.36 | ppl   579.50\n",
      "| epoch   3 |    84/  186 batches | lr 4.29 | ms/batch 79.32 |  ms/sample 0.24786 | loss  6.34 | ppl   566.60\n",
      "| epoch   3 |    96/  186 batches | lr 4.29 | ms/batch 78.39 |  ms/sample 0.24498 | loss  6.29 | ppl   541.60\n",
      "| epoch   3 |   108/  186 batches | lr 4.29 | ms/batch 78.70 |  ms/sample 0.24594 | loss  6.29 | ppl   539.08\n",
      "| epoch   3 |   120/  186 batches | lr 4.29 | ms/batch 78.29 |  ms/sample 0.24466 | loss  6.24 | ppl   512.16\n",
      "| epoch   3 |   132/  186 batches | lr 4.29 | ms/batch 78.27 |  ms/sample 0.24459 | loss  6.20 | ppl   492.65\n",
      "| epoch   3 |   144/  186 batches | lr 4.29 | ms/batch 78.17 |  ms/sample 0.24428 | loss  6.26 | ppl   521.18\n",
      "| epoch   3 |   156/  186 batches | lr 4.29 | ms/batch 78.08 |  ms/sample 0.24399 | loss  6.26 | ppl   524.58\n",
      "| epoch   3 |   168/  186 batches | lr 4.29 | ms/batch 78.05 |  ms/sample 0.24390 | loss  6.22 | ppl   502.24\n",
      "| epoch   3 |   180/  186 batches | lr 4.29 | ms/batch 78.17 |  ms/sample 0.24429 | loss  6.20 | ppl   491.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 15.84s | valid loss  5.98 | valid ppl   394.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.91 | test ppl   368.28\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "main(320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
